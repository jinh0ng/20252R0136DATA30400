{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e76789-0177-47ea-887c-aa8df8a828ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:10:22.969756Z",
     "iopub.status.busy": "2025-12-19T06:10:22.969340Z",
     "iopub.status.idle": "2025-12-19T06:10:22.975420Z",
     "shell.execute_reply": "2025-12-19T06:10:22.974883Z",
     "shell.execute_reply.started": "2025-12-19T06:10:22.969736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/sagemaker-user\n",
      "DATA_ROOT: /home/sagemaker-user/project_release/Amazon_products exists: True\n",
      "classes -> /home/sagemaker-user/project_release/Amazon_products/classes.txt exists: True size: 9646\n",
      "hier -> /home/sagemaker-user/project_release/Amazon_products/class_hierarchy.txt exists: True size: 4086\n",
      "kw -> /home/sagemaker-user/project_release/Amazon_products/class_related_keywords.txt exists: True size: 87468\n",
      "train -> /home/sagemaker-user/project_release/Amazon_products/train/train_corpus.txt exists: True size: 14125831\n",
      "test -> /home/sagemaker-user/project_release/Amazon_products/test/test_corpus.txt exists: True size: 9428649\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, re, json\n",
    "import pandas as pd\n",
    "\n",
    "CWD = Path.cwd()\n",
    "DATA_ROOT = CWD / \"project_release\" / \"Amazon_products\"\n",
    "\n",
    "paths = {\n",
    "    \"classes\": DATA_ROOT / \"classes.txt\",\n",
    "    \"hier\":    DATA_ROOT / \"class_hierarchy.txt\",\n",
    "    \"kw\":      DATA_ROOT / \"class_related_keywords.txt\",\n",
    "    \"train\":   DATA_ROOT / \"train\" / \"train_corpus.txt\",\n",
    "    \"test\":    DATA_ROOT / \"test\"  / \"test_corpus.txt\",\n",
    "}\n",
    "\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"DATA_ROOT:\", DATA_ROOT, \"exists:\", DATA_ROOT.exists())\n",
    "for k,v in paths.items():\n",
    "    print(k, \"->\", v, \"exists:\", v.exists(), \"size:\", (v.stat().st_size if v.exists() else None))\n",
    "\n",
    "# 강제 체크(하나라도 없으면 여기서 멈추기)\n",
    "missing = [k for k,v in paths.items() if not v.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing files: {missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf56c2d8-06dc-4965-b2aa-542ec59baabb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:10:30.163685Z",
     "iopub.status.busy": "2025-12-19T06:10:30.163414Z",
     "iopub.status.idle": "2025-12-19T06:10:30.168876Z",
     "shell.execute_reply": "2025-12-19T06:10:30.168344Z",
     "shell.execute_reply.started": "2025-12-19T06:10:30.163664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== classes ====\n",
      "01: 0\tgrocery_gourmet_food\n",
      "02: 1\tmeat_poultry\n",
      "03: 2\tjerky\n",
      "04: 3\ttoys_games\n",
      "05: 4\tgames\n",
      "06: 5\tpuzzles\n",
      "07: 6\tjigsaw_puzzles\n",
      "08: 7\tboard_games\n",
      "\n",
      "==== hier ====\n",
      "01: 0\t1\n",
      "02: 0\t8\n",
      "03: 0\t208\n",
      "04: 0\t211\n",
      "05: 0\t213\n",
      "06: 0\t216\n",
      "07: 0\t229\n",
      "08: 0\t255\n",
      "\n",
      "==== kw ====\n",
      "01: grocery_gourmet_food:snacks,condiments,beverages,specialty_foods,spices,cooking_oils,baking_ingredients,gourmet_chocolates,artisanal_cheeses,organic_foods\n",
      "02: meat_poultry:butcher,cuts,marination,grilling,roasting,seasoning,halal,organic,deli,marbling\n",
      "03: jerky:beef,turkey,chicken,venison,buffalo,kangaroo,elk,ostrich,bison,spicy\n",
      "04: toys_games:board_games,puzzles,action_figures,building_blocks,dolls,outdoor_toys,educational_toys,card_games,remote_control_toys,plush_toys\n",
      "05: games:board_games,card_games,tabletop_games,party_games,roleplaying_games,video_games,strategy_games,family_games,word_games,dice_games\n",
      "06: puzzles:jigsaw_puzzles,brain_teasers,puzzle_accessories,puzzle_storage,puzzle_mats,puzzle_glue,puzzle_organizers,puzzle_books,puzzle_magazines,puzzle_competitions\n",
      "07: jigsaw_puzzles:interlocking_pieces,puzzle_boards,puzzle_glue,puzzle_storage,puzzle_frames,puzzle_rolls,puzzle_organizers,puzzle_tables,puzzle_sleeves,puzzle_sorting_trays\n",
      "08: board_games:board_game_accessories,strategy_games,cooperative_games,family_games,classic_board_games,party_games,educational_board_games,roleplaying_games,abstract_strategy_games,word_games\n",
      "\n",
      "==== train ====\n",
      "01: 0\tomron hem 790it automatic blood pressure monitor with advanced omron health management software so far this machine has worked well and is very simple to use . it is nice to have immediate feedback on the bloodpressure effects of my various exercis\n",
      "02: 1\tnatural factors whey factors chocolate works well , but there is a lot of dead space in the container when you first open it up . the container comes 3 4 4 5 full and the rest in empty space .\n",
      "03: 2\tclif bar builder 's bar , 2 . 4 ounce bars i love the peanut butter builder 's bars . while amazon is great for so many things , a trip to tj 's is too good to pass up . a little cup of coffee , a sample or two and into the cart with some fresh veg\n",
      "04: 3\tandis 1875 watt professional ceramic ionic hair dryer i was a little hesitant to purchase since it was rather cheap for a good ionic dryer but it worked great . i have 3c type hair and it dried my hair rather quickly and straight , with minimal fri\n",
      "05: 4\tclif bar energy bars these were cheaper than what i had bought at sam 's and worked very well . they are very convient and i would recommend them to anyone not willing to make their own at a cheaper price because of the convience of having a good p\n",
      "06: 5\tgillette fusion power cartridges until i tried fusion catridges , i got decent shaves but not great . i was always cutting myself atleast once a week , especially on my chin . now , i get the smooth shaves and i never cut myself . i also think the \n",
      "07: 6\tfisher price bright beginnings stacking action blocks we played with these at a friend 's house when our son was about 10 months old . he really enjoyed playing with them then . we bought him his own set for his first birthday . he played with them\n",
      "08: 7\tsony rechargeable battery part npfe1 the replacement battery charged in less time then the oem . excellent value and allowed me to keep my older digital camera . fe1 battery kept 10mp camera fully functionalwhen charged with new digital ac dc charg\n",
      "\n",
      "==== test ====\n",
      "01: 0\tconair cs15tcs professional straight styles straightening iron woah ! sure this straightener looks like all the other crappy straightners in the world , but there 's a twist to this one ! it is my first straightner and i 've had it for about 7 mont\n",
      "02: 1\tbarbie ballet shoes icon doll i was looking round the toysrus website and found this cheap doll ! \" wow \" i said . so i got it and her body is painted on ! which is really cute ! , parents would n't you like to get a toy where you save yourself fro\n",
      "03: 2\tcloud b twilight constellation night light i bought this item because it had good reviews , and looked really cool . it 's a turtle with 3 different colored led lights on the inside that are supposed to project stars in a room or on a ceiling . of \n",
      "04: 3\talessi zuppa toscana tuscan white bean soup ( pack of 6 ) this soup taste great . even better if you add the olive oil and grated parm cheese . only takes about 15 mins to make . the package arrived quick . ordered on sunday night , had tuesday mor\n",
      "05: 4\tswedish beauty amaretto tanning lotion advanced dark tanning system with triple bronzer and shimmer i ordered this because i was running low on my current tanning lotion which i love and it smells like skittles , so i read the reviews for this one \n",
      "06: 5\tstash green white tea blends , 18 or 20 count tea bags ( pack of 6 ) i 've been tasting along several green tea 's trying to find one that i liked . i was beginning to feel a little like goldilocks . one would be too strong , the other too bland . \n",
      "07: 6\tmama mio tummy rub stretch mark butter , 4 oz , say no to stretch marks ! i started with the mama mio oil and loved it so much i had to try the belly butter . it 's wonderful . smells great and absorbs really well . on days when i really want to ma\n",
      "08: 7\ttiny love super deluxe lights and music gymini activity gym we gifted this toy to our niece nimisha when she was completing 2 months .. initially she spent only few minutes in this gym .. but as she grows she seems fascinated about the gym and abso\n"
     ]
    }
   ],
   "source": [
    "def head_lines(path, n=8):\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for _ in range(n):\n",
    "            out.append(next(f).rstrip(\"\\n\"))\n",
    "    return out\n",
    "\n",
    "for k in [\"classes\", \"hier\", \"kw\", \"train\", \"test\"]:\n",
    "    print(\"\\n====\", k, \"====\")\n",
    "    lines = head_lines(paths[k], n=8)\n",
    "    for i,l in enumerate(lines, 1):\n",
    "        print(f\"{i:02d}: {l[:250]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c29c78-1823-4d7b-afa9-0b2de8bf5d1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:10:51.238796Z",
     "iopub.status.busy": "2025-12-19T06:10:51.238524Z",
     "iopub.status.idle": "2025-12-19T06:10:51.258043Z",
     "shell.execute_reply": "2025-12-19T06:10:51.257509Z",
     "shell.execute_reply.started": "2025-12-19T06:10:51.238775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_df shape: (531, 3)\n",
      "raw_id null count: 0\n",
      "duplicate class_idx: 0\n",
      "duplicate class_name: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>raw_id</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>grocery_gourmet_food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>meat_poultry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>jerky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>toys_games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>puzzles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>jigsaw_puzzles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>board_games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>juices</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_idx  raw_id            class_name\n",
       "0          0       0  grocery_gourmet_food\n",
       "1          1       1          meat_poultry\n",
       "2          2       2                 jerky\n",
       "3          3       3            toys_games\n",
       "4          4       4                 games\n",
       "5          5       5               puzzles\n",
       "6          6       6        jigsaw_puzzles\n",
       "7          7       7           board_games\n",
       "8          8       8             beverages\n",
       "9          9       9                juices"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>raw_id</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>521</td>\n",
       "      <td>521</td>\n",
       "      <td>spices_gifts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>522</td>\n",
       "      <td>522</td>\n",
       "      <td>dried_fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>523</td>\n",
       "      <td>523</td>\n",
       "      <td>flying_toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>524</td>\n",
       "      <td>524</td>\n",
       "      <td>shampoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>525</td>\n",
       "      <td>525</td>\n",
       "      <td>coatings_batters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>526</td>\n",
       "      <td>526</td>\n",
       "      <td>hydrometers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>527</td>\n",
       "      <td>527</td>\n",
       "      <td>lamb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>528</td>\n",
       "      <td>528</td>\n",
       "      <td>exercise_wheels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>529</td>\n",
       "      <td>529</td>\n",
       "      <td>chocolate_covered_nuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "      <td>breeding_tanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_idx  raw_id              class_name\n",
       "521        521     521            spices_gifts\n",
       "522        522     522             dried_fruit\n",
       "523        523     523             flying_toys\n",
       "524        524     524                 shampoo\n",
       "525        525     525        coatings_batters\n",
       "526        526     526             hydrometers\n",
       "527        527     527                    lamb\n",
       "528        528     528         exercise_wheels\n",
       "529        529     529  chocolate_covered_nuts\n",
       "530        530     530          breeding_tanks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 531\n"
     ]
    }
   ],
   "source": [
    "def parse_classes(lines):\n",
    "    rows = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        m = re.match(r\"^\\s*(\\d+)\\s*[\\t,]\\s*(.+?)\\s*$\", s)\n",
    "        if m:\n",
    "            raw_id = int(m.group(1))\n",
    "            name = m.group(2).strip()\n",
    "        else:\n",
    "            raw_id = None\n",
    "            name = s\n",
    "        rows.append((raw_id, name))\n",
    "\n",
    "    has_ids = sum(r[0] is not None for r in rows) >= (len(rows)*0.8)\n",
    "\n",
    "    if has_ids:\n",
    "        seen = {}\n",
    "        mapped = []\n",
    "        for raw_id, name in rows:\n",
    "            if raw_id not in seen:\n",
    "                seen[raw_id] = len(seen)\n",
    "            mapped.append((seen[raw_id], raw_id, name))\n",
    "    else:\n",
    "        mapped = [(i, raw_id, name) for i,(raw_id,name) in enumerate(rows)]\n",
    "\n",
    "    df = pd.DataFrame(mapped, columns=[\"class_idx\",\"raw_id\",\"class_name\"])\n",
    "    df[\"class_name\"] = df[\"class_name\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "with open(paths[\"classes\"], \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    class_lines = [l.rstrip(\"\\n\") for l in f]\n",
    "\n",
    "classes_df = parse_classes(class_lines)\n",
    "\n",
    "print(\"classes_df shape:\", classes_df.shape)\n",
    "print(\"raw_id null count:\", classes_df[\"raw_id\"].isna().sum())\n",
    "print(\"duplicate class_idx:\", classes_df[\"class_idx\"].duplicated().sum())\n",
    "print(\"duplicate class_name:\", classes_df[\"class_name\"].duplicated().sum())\n",
    "display(classes_df.head(10))\n",
    "display(classes_df.tail(10))\n",
    "\n",
    "# 기대: 531개인지 확인(과제 스펙)\n",
    "print(\"num_classes:\", len(classes_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839b854-901c-4fb5-a20c-39681e5da389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:11:06.156505Z",
     "iopub.status.busy": "2025-12-19T06:11:06.156211Z",
     "iopub.status.idle": "2025-12-19T06:11:06.166337Z",
     "shell.execute_reply": "2025-12-19T06:11:06.165851Z",
     "shell.execute_reply.started": "2025-12-19T06:11:06.156484Z"
    }
   },
   "outputs": [],
   "source": [
    "# raw_id -> class_idx 매핑(가능한 경우)\n",
    "rawid_to_idx = {}\n",
    "if classes_df[\"raw_id\"].notna().all():\n",
    "    rawid_to_idx = {int(r.raw_id): int(r.class_idx) for r in classes_df.itertuples()}\n",
    "\n",
    "def parse_edges(lines):\n",
    "    edges_raw = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        parts = re.split(r\"[\\t, ]+\", s)\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        edges_raw.append((parts[0], parts[1]))\n",
    "    return edges_raw\n",
    "\n",
    "with open(paths[\"hier\"], \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    hier_lines = [l.rstrip(\"\\n\") for l in f]\n",
    "\n",
    "edges_raw = parse_edges(hier_lines)\n",
    "print(\"raw edges:\", len(edges_raw), \"sample:\", edges_raw[:5])\n",
    "\n",
    "edges = []\n",
    "unmapped = 0\n",
    "for a,b in edges_raw:\n",
    "    try:\n",
    "        pa = int(a); ch = int(b)\n",
    "        if rawid_to_idx:\n",
    "            if pa in rawid_to_idx and ch in rawid_to_idx:\n",
    "                edges.append((rawid_to_idx[pa], rawid_to_idx[ch]))\n",
    "            else:\n",
    "                unmapped += 1\n",
    "        else:\n",
    "            edges.append((pa, ch))\n",
    "    except:\n",
    "        unmapped += 1\n",
    "\n",
    "print(\"mapped edges:\", len(edges), \"unmapped:\", unmapped)\n",
    "print(\"mapped sample:\", edges[:5])\n",
    "\n",
    "from collections import defaultdict\n",
    "children = defaultdict(list)\n",
    "parents  = defaultdict(list)\n",
    "for p,c in edges:\n",
    "    children[p].append(c)\n",
    "    parents[c].append(p)\n",
    "\n",
    "all_nodes = set(classes_df[\"class_idx\"].tolist())\n",
    "has_parent = set(parents.keys())\n",
    "roots = sorted(list(all_nodes - has_parent))\n",
    "\n",
    "print(\"num nodes:\", len(all_nodes))\n",
    "print(\"num roots candidate:\", len(roots))\n",
    "print(\"roots sample:\", roots[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6ac141-6261-4551-aabb-cc1b764a747a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:12:49.145849Z",
     "iopub.status.busy": "2025-12-19T06:12:49.145599Z",
     "iopub.status.idle": "2025-12-19T06:12:49.156725Z",
     "shell.execute_reply": "2025-12-19T06:12:49.156193Z",
     "shell.execute_reply.started": "2025-12-19T06:12:49.145830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KW mapped classes: 531\n",
      "Bad kw lines: 0\n",
      "Classes without keywords: 0\n",
      "[0] grocery_gourmet_food -> ['snacks', 'condiments', 'beverages', 'specialty_foods', 'spices', 'cooking_oils', 'baking_ingredients', 'gourmet_chocolates', 'artisanal_cheeses', 'organic_foods']\n",
      "[1] meat_poultry -> ['butcher', 'cuts', 'marination', 'grilling', 'roasting', 'seasoning', 'halal', 'organic', 'deli', 'marbling']\n",
      "[2] jerky -> ['beef', 'turkey', 'chicken', 'venison', 'buffalo', 'kangaroo', 'elk', 'ostrich', 'bison', 'spicy']\n",
      "[3] toys_games -> ['board_games', 'puzzles', 'action_figures', 'building_blocks', 'dolls', 'outdoor_toys', 'educational_toys', 'card_games', 'remote_control_toys', 'plush_toys']\n",
      "[4] games -> ['board_games', 'card_games', 'tabletop_games', 'party_games', 'roleplaying_games', 'video_games', 'strategy_games', 'family_games', 'word_games', 'dice_games']\n",
      "[5] puzzles -> ['jigsaw_puzzles', 'brain_teasers', 'puzzle_accessories', 'puzzle_storage', 'puzzle_mats', 'puzzle_glue', 'puzzle_organizers', 'puzzle_books', 'puzzle_magazines', 'puzzle_competitions']\n",
      "[6] jigsaw_puzzles -> ['interlocking_pieces', 'puzzle_boards', 'puzzle_glue', 'puzzle_storage', 'puzzle_frames', 'puzzle_rolls', 'puzzle_organizers', 'puzzle_tables', 'puzzle_sleeves', 'puzzle_sorting_trays']\n",
      "[7] board_games -> ['board_game_accessories', 'strategy_games', 'cooperative_games', 'family_games', 'classic_board_games', 'party_games', 'educational_board_games', 'roleplaying_games', 'abstract_strategy_games', 'word_games']\n",
      "No-keyword class_idx sample: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# class_name -> class_idx\n",
    "name_to_idx = {r.class_name: int(r.class_idx) for r in classes_df.itertuples()}\n",
    "\n",
    "def parse_kw_by_name(lines):\n",
    "    kw_map = {}\n",
    "    bad = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if \":\" not in s:\n",
    "            bad.append(s)\n",
    "            continue\n",
    "        left, right = s.split(\":\", 1)\n",
    "        cls_name = left.strip()\n",
    "        if cls_name not in name_to_idx:\n",
    "            bad.append(s)\n",
    "            continue\n",
    "        # 키워드 토큰화: 콤마 중심\n",
    "        kws = [t.strip() for t in right.split(\",\") if t.strip()]\n",
    "        kw_map[name_to_idx[cls_name]] = kws\n",
    "    return kw_map, bad\n",
    "\n",
    "with open(paths[\"kw\"], \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    kw_lines = [l.rstrip(\"\\n\") for l in f]\n",
    "\n",
    "kw_map, bad_kw_lines = parse_kw_by_name(kw_lines)\n",
    "\n",
    "print(\"KW mapped classes:\", len(kw_map))\n",
    "print(\"Bad kw lines:\", len(bad_kw_lines))\n",
    "print(\"Classes without keywords:\", len(classes_df) - len(kw_map))\n",
    "\n",
    "# 샘플 확인\n",
    "for ci in list(kw_map.keys())[:8]:\n",
    "    name = classes_df.loc[classes_df.class_idx==ci, \"class_name\"].iloc[0]\n",
    "    print(f\"[{ci}] {name} -> {kw_map[ci][:12]}\")\n",
    "\n",
    "# 키워드 없는 클래스 몇 개 확인(리포트/디버깅용)\n",
    "no_kw = sorted(list(set(classes_df.class_idx) - set(kw_map.keys())))\n",
    "print(\"No-keyword class_idx sample:\", no_kw[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "311fedd4-1055-432b-b490-ec74df8f40a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:11:37.428072Z",
     "iopub.status.busy": "2025-12-19T06:11:37.427668Z",
     "iopub.status.idle": "2025-12-19T06:11:37.573300Z",
     "shell.execute_reply": "2025-12-19T06:11:37.572789Z",
     "shell.execute_reply.started": "2025-12-19T06:11:37.428038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (29487, 3) {'tab_ratio': 1.0, 'comma_ratio': 0.82, 'chosen_delim': '\\t'}\n",
      "  len(min/mean/max): 94 472.42961304981856 8147\n",
      "test : (19658, 3) {'tab_ratio': 1.0, 'comma_ratio': 0.76, 'chosen_delim': '\\t'}\n",
      "  len(min/mean/max): 81 473.1993590395768 6797\n",
      "train duplicate ids: 0\n",
      "test duplicate ids : 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>omron hem 790it automatic blood pressure monit...</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>natural factors whey factors chocolate works w...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>clif bar builder 's bar , 2 . 4 ounce bars i l...</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>andis 1875 watt professional ceramic ionic hai...</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>clif bar energy bars these were cheaper than w...</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id                                               text  text_len\n",
       "0      0  omron hem 790it automatic blood pressure monit...       305\n",
       "1      1  natural factors whey factors chocolate works w...       192\n",
       "2      2  clif bar builder 's bar , 2 . 4 ounce bars i l...       561\n",
       "3      3  andis 1875 watt professional ceramic ionic hai...       367\n",
       "4      4  clif bar energy bars these were cheaper than w...       638"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>conair cs15tcs professional straight styles st...</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>barbie ballet shoes icon doll i was looking ro...</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>cloud b twilight constellation night light i b...</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>alessi zuppa toscana tuscan white bean soup ( ...</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>swedish beauty amaretto tanning lotion advance...</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id                                               text  text_len\n",
       "0      0  conair cs15tcs professional straight styles st...       501\n",
       "1      1  barbie ballet shoes icon doll i was looking ro...       479\n",
       "2      2  cloud b twilight constellation night light i b...       730\n",
       "3      3  alessi zuppa toscana tuscan white bean soup ( ...       345\n",
       "4      4  swedish beauty amaretto tanning lotion advance...       540"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_corpus(path, n_probe=200):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        probe = []\n",
    "        for _ in range(n_probe):\n",
    "            try:\n",
    "                probe.append(next(f).rstrip(\"\\n\"))\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "    tab_ratio = (sum(\"\\t\" in l for l in probe) / len(probe)) if probe else 0.0\n",
    "    comma_ratio = (sum(\",\" in l for l in probe) / len(probe)) if probe else 0.0\n",
    "    delim = \"\\t\" if tab_ratio >= max(comma_ratio, 0.2) else (\",\" if comma_ratio >= 0.2 else None)\n",
    "\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for i,ln in enumerate(f):\n",
    "            s = ln.rstrip(\"\\n\")\n",
    "            if not s.strip():\n",
    "                continue\n",
    "            if delim and delim in s:\n",
    "                a,b = s.split(delim, 1)\n",
    "                a=a.strip(); b=b.strip()\n",
    "                if re.match(r\"^[A-Za-z0-9_\\-]+$\", a) and len(b) > 0:\n",
    "                    doc_id, text = a, b\n",
    "                else:\n",
    "                    doc_id, text = f\"auto_{i:08d}\", s\n",
    "            else:\n",
    "                doc_id, text = f\"auto_{i:08d}\", s\n",
    "            rows.append((doc_id, text))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"doc_id\",\"text\"])\n",
    "    df[\"text_len\"] = df[\"text\"].astype(str).str.len()\n",
    "    probe_info = {\"tab_ratio\": tab_ratio, \"comma_ratio\": comma_ratio, \"chosen_delim\": delim}\n",
    "    return df, probe_info\n",
    "\n",
    "train_df, train_probe = load_corpus(paths[\"train\"])\n",
    "test_df,  test_probe  = load_corpus(paths[\"test\"])\n",
    "\n",
    "print(\"train:\", train_df.shape, train_probe)\n",
    "print(\"  len(min/mean/max):\", int(train_df.text_len.min()), float(train_df.text_len.mean()), int(train_df.text_len.max()))\n",
    "print(\"test :\", test_df.shape, test_probe)\n",
    "print(\"  len(min/mean/max):\", int(test_df.text_len.min()), float(test_df.text_len.mean()), int(test_df.text_len.max()))\n",
    "\n",
    "print(\"train duplicate ids:\", int(train_df.doc_id.duplicated().sum()))\n",
    "print(\"test duplicate ids :\", int(test_df.doc_id.duplicated().sum()))\n",
    "\n",
    "display(train_df.head(5))\n",
    "display(test_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1deb9ca-016c-4abb-b110-15b6b8d28c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:14:30.937690Z",
     "iopub.status.busy": "2025-12-19T06:14:30.937307Z",
     "iopub.status.idle": "2025-12-19T06:14:30.941835Z",
     "shell.execute_reply": "2025-12-19T06:14:30.941306Z",
     "shell.execute_reply.started": "2025-12-19T06:14:30.937668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLASSES: 531\n",
      "NUM_EDGES: 568\n",
      "KW_COVERAGE(classes with keywords): 531\n",
      "{'paths': {'classes': '/home/sagemaker-user/project_release/Amazon_products/classes.txt', 'hier': '/home/sagemaker-user/project_release/Amazon_products/class_hierarchy.txt', 'kw': '/home/sagemaker-user/project_release/Amazon_products/class_related_keywords.txt', 'train': '/home/sagemaker-user/project_release/Amazon_products/train/train_corpus.txt', 'test': '/home/sagemaker-user/project_release/Amazon_products/test/test_corpus.txt'}, 'num_classes': 531, 'num_edges': 568, 'kw_coverage': 531, 'num_train': 29487, 'num_test': 19658}\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(classes_df)\n",
    "NUM_EDGES = len(edges)\n",
    "KW_COVERAGE = len(kw_map)\n",
    "\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "print(\"NUM_EDGES:\", NUM_EDGES)\n",
    "print(\"KW_COVERAGE(classes with keywords):\", KW_COVERAGE)\n",
    "\n",
    "step1 = {\n",
    "    \"paths\": {k: str(v) for k,v in paths.items()},\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "    \"num_edges\": NUM_EDGES,\n",
    "    \"kw_coverage\": KW_COVERAGE,\n",
    "    \"num_train\": len(train_df),\n",
    "    \"num_test\": len(test_df),\n",
    "}\n",
    "print(step1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8694aa-c4c4-4124-8761-a826e4e251d0",
   "metadata": {},
   "source": [
    "### Step 2: class label 문서 만들기. (추후 silver label 생성에 사용할 라벨 설명 text만드는 것 목표)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef258c80-6c2c-44db-97e5-60cea875c036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:19:17.057586Z",
     "iopub.status.busy": "2025-12-19T06:19:17.057338Z",
     "iopub.status.idle": "2025-12-19T06:19:17.061325Z",
     "shell.execute_reply": "2025-12-19T06:19:17.060828Z",
     "shell.execute_reply.started": "2025-12-19T06:19:17.057566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP_KW: 12 | USE_PRETTY_NAME: True | INCLUDE_PARENT_PATH: True\n"
     ]
    }
   ],
   "source": [
    "# Step 2 config\n",
    "TOP_KW = 12              # 클래스당 사용할 키워드 수(너무 많으면 노이즈)\n",
    "USE_PRETTY_NAME = True   # underscore -> space 변환(임베딩 매칭 시 유리)\n",
    "INCLUDE_PARENT_PATH = True  # taxonomy를 텍스트에 약하게 주입(선택)\n",
    "\n",
    "# Step1 산출물이 존재하는지 체크\n",
    "assert \"classes_df\" in globals()\n",
    "assert \"kw_map\" in globals()\n",
    "assert \"edges\" in globals()\n",
    "assert \"train_df\" in globals()\n",
    "assert \"test_df\" in globals()\n",
    "\n",
    "print(\"TOP_KW:\", TOP_KW, \"| USE_PRETTY_NAME:\", USE_PRETTY_NAME, \"| INCLUDE_PARENT_PATH:\", INCLUDE_PARENT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a9a5a77-f0ab-4e50-82a3-8ada1eaf1ca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:19:23.334638Z",
     "iopub.status.busy": "2025-12-19T06:19:23.334351Z",
     "iopub.status.idle": "2025-12-19T06:19:23.340372Z",
     "shell.execute_reply": "2025-12-19T06:19:23.339828Z",
     "shell.execute_reply.started": "2025-12-19T06:19:23.334615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots: [0, 3, 10, 23, 40, 169]\n",
      "0 parents: [] chain: []\n",
      "1 parents: [0] chain: [0]\n",
      "8 parents: [0] chain: [0]\n",
      "23 parents: [] chain: []\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "parents = defaultdict(list)\n",
    "children = defaultdict(list)\n",
    "for p, c in edges:\n",
    "    children[p].append(c)\n",
    "    parents[c].append(p)\n",
    "\n",
    "# 루트 후보(부모 없는 노드)\n",
    "all_nodes = set(classes_df[\"class_idx\"].tolist())\n",
    "roots = sorted(list(all_nodes - set(parents.keys())))\n",
    "print(\"roots:\", roots)\n",
    "\n",
    "# 부모 체인(한 클래스에 부모가 여러 개일 수 있으므로 '대표 경로'를 하나만 택하는 간단 규칙)\n",
    "# 규칙: 부모가 여러 개면 가장 작은 idx를 택해 위로 올라감(결정적(deterministic)인 baseline)\n",
    "def get_parent_chain(ci, max_hops=50):\n",
    "    chain = []\n",
    "    cur = ci\n",
    "    hops = 0\n",
    "    while cur in parents and len(parents[cur]) > 0 and hops < max_hops:\n",
    "        ps = sorted(parents[cur])\n",
    "        cur = ps[0]\n",
    "        chain.append(cur)\n",
    "        hops += 1\n",
    "    return chain  # [parent, grandparent, ...] (위로)\n",
    "\n",
    "# 샘플 확인\n",
    "for ci in [0, 1, 8, 23]:\n",
    "    print(ci, \"parents:\", parents.get(ci, []), \"chain:\", get_parent_chain(ci))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dabc9b4-d005-44e9-8cdf-9f90ea8ef1ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:19:42.210541Z",
     "iopub.status.busy": "2025-12-19T06:19:42.210234Z",
     "iopub.status.idle": "2025-12-19T06:19:42.227998Z",
     "shell.execute_reply": "2025-12-19T06:19:42.227508Z",
     "shell.execute_reply.started": "2025-12-19T06:19:42.210520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_df shape: (531, 5)\n",
      "empty label_text: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class_name</th>\n",
       "      <th>num_keywords_used</th>\n",
       "      <th>label_text</th>\n",
       "      <th>has_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>grocery_gourmet_food</td>\n",
       "      <td>10</td>\n",
       "      <td>grocery gourmet food. keywords: snacks, condim...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>meat_poultry</td>\n",
       "      <td>10</td>\n",
       "      <td>meat poultry. keywords: butcher, cuts, marinat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>jerky</td>\n",
       "      <td>10</td>\n",
       "      <td>jerky. keywords: beef, turkey, chicken, veniso...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>toys_games</td>\n",
       "      <td>10</td>\n",
       "      <td>toys games. keywords: board_games, puzzles, ac...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>games</td>\n",
       "      <td>10</td>\n",
       "      <td>games. keywords: board_games, card_games, tabl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>puzzles</td>\n",
       "      <td>10</td>\n",
       "      <td>puzzles. keywords: jigsaw_puzzles, brain_tease...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>jigsaw_puzzles</td>\n",
       "      <td>10</td>\n",
       "      <td>jigsaw puzzles. keywords: interlocking_pieces,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>board_games</td>\n",
       "      <td>10</td>\n",
       "      <td>board games. keywords: board_game_accessories,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_idx            class_name  num_keywords_used  \\\n",
       "0          0  grocery_gourmet_food                 10   \n",
       "1          1          meat_poultry                 10   \n",
       "2          2                 jerky                 10   \n",
       "3          3            toys_games                 10   \n",
       "4          4                 games                 10   \n",
       "5          5               puzzles                 10   \n",
       "6          6        jigsaw_puzzles                 10   \n",
       "7          7           board_games                 10   \n",
       "\n",
       "                                          label_text  has_path  \n",
       "0  grocery gourmet food. keywords: snacks, condim...     False  \n",
       "1  meat poultry. keywords: butcher, cuts, marinat...      True  \n",
       "2  jerky. keywords: beef, turkey, chicken, veniso...      True  \n",
       "3  toys games. keywords: board_games, puzzles, ac...     False  \n",
       "4  games. keywords: board_games, card_games, tabl...      True  \n",
       "5  puzzles. keywords: jigsaw_puzzles, brain_tease...      True  \n",
       "6  jigsaw puzzles. keywords: interlocking_pieces,...      True  \n",
       "7  board games. keywords: board_game_accessories,...      True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_idx</th>\n",
       "      <th>class_name</th>\n",
       "      <th>num_keywords_used</th>\n",
       "      <th>label_text</th>\n",
       "      <th>has_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>409</td>\n",
       "      <td>bars</td>\n",
       "      <td>10</td>\n",
       "      <td>bars. keywords: protein_bars, energy_bars, gra...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>438</td>\n",
       "      <td>halva</td>\n",
       "      <td>10</td>\n",
       "      <td>halva. keywords: sesame, tahini, middle_easter...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>268</td>\n",
       "      <td>slot_cars</td>\n",
       "      <td>10</td>\n",
       "      <td>slot cars. keywords: racing_tracks, digital_sl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>joggers</td>\n",
       "      <td>10</td>\n",
       "      <td>joggers. keywords: running, fitness, outdoors,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>450</td>\n",
       "      <td>memorials</td>\n",
       "      <td>10</td>\n",
       "      <td>memorials. keywords: grave_markers, urns, memo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>283</td>\n",
       "      <td>bathroom_aids_safety</td>\n",
       "      <td>10</td>\n",
       "      <td>bathroom aids safety. keywords: shower_chairs,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>281</td>\n",
       "      <td>training_behavior_aids</td>\n",
       "      <td>10</td>\n",
       "      <td>training behavior aids. keywords: clickers, le...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>action_toy_figures</td>\n",
       "      <td>10</td>\n",
       "      <td>action toy figures. keywords: poseable_figures...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_idx              class_name  num_keywords_used  \\\n",
       "409        409                    bars                 10   \n",
       "438        438                   halva                 10   \n",
       "268        268               slot_cars                 10   \n",
       "196        196                 joggers                 10   \n",
       "450        450               memorials                 10   \n",
       "283        283    bathroom_aids_safety                 10   \n",
       "281        281  training_behavior_aids                 10   \n",
       "15          15      action_toy_figures                 10   \n",
       "\n",
       "                                            label_text  has_path  \n",
       "409  bars. keywords: protein_bars, energy_bars, gra...      True  \n",
       "438  halva. keywords: sesame, tahini, middle_easter...      True  \n",
       "268  slot cars. keywords: racing_tracks, digital_sl...      True  \n",
       "196  joggers. keywords: running, fitness, outdoors,...      True  \n",
       "450  memorials. keywords: grave_markers, urns, memo...      True  \n",
       "283  bathroom aids safety. keywords: shower_chairs,...      True  \n",
       "281  training behavior aids. keywords: clickers, le...      True  \n",
       "15   action toy figures. keywords: poseable_figures...      True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pretty(name: str) -> str:\n",
    "    return name.replace(\"_\", \" \") if USE_PRETTY_NAME else name\n",
    "\n",
    "# class_idx -> class_name\n",
    "idx_to_name = {int(r.class_idx): r.class_name for r in classes_df.itertuples()}\n",
    "\n",
    "label_text = {}\n",
    "label_meta = []  # DataFrame용\n",
    "\n",
    "for ci in range(len(classes_df)):\n",
    "    cname = idx_to_name[ci]\n",
    "    kws = kw_map.get(ci, [])\n",
    "    kws = kws[:TOP_KW]\n",
    "\n",
    "    # parent path 텍스트(선택)\n",
    "    path_txt = \"\"\n",
    "    if INCLUDE_PARENT_PATH:\n",
    "        chain = get_parent_chain(ci)\n",
    "        # chain은 부모부터 위로; 사람이 읽기 좋게 root->...->parent 형태로 뒤집기\n",
    "        chain_names = [pretty(idx_to_name[x]) for x in reversed(chain)]\n",
    "        if chain_names:\n",
    "            path_txt = \" | path: \" + \" > \".join(chain_names)\n",
    "\n",
    "    text = f\"{pretty(cname)}. keywords: {', '.join(kws)}{path_txt}\"\n",
    "    label_text[ci] = text\n",
    "\n",
    "    label_meta.append({\n",
    "        \"class_idx\": ci,\n",
    "        \"class_name\": cname,\n",
    "        \"num_keywords_used\": len(kws),\n",
    "        \"label_text\": text,\n",
    "        \"has_path\": bool(path_txt),\n",
    "    })\n",
    "\n",
    "label_df = pd.DataFrame(label_meta)\n",
    "\n",
    "print(\"label_df shape:\", label_df.shape)\n",
    "print(\"empty label_text:\", int((label_df[\"label_text\"].str.len() == 0).sum()))\n",
    "display(label_df.head(8))\n",
    "display(label_df.sample(8, random_state=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbdb945b-95a6-43c1-8a39-e8f048f23e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:20:11.941123Z",
     "iopub.status.busy": "2025-12-19T06:20:11.940858Z",
     "iopub.status.idle": "2025-12-19T06:20:11.947130Z",
     "shell.execute_reply": "2025-12-19T06:20:11.946644Z",
     "shell.execute_reply.started": "2025-12-19T06:20:11.941103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_text len(min/mean/max): 131 220.58757062146893 350\n",
      "keywords used(min/mean/max): 1 9.932203389830509 10\n",
      "\n",
      "--- class_idx 0 | grocery_gourmet_food ---\n",
      "grocery gourmet food. keywords: snacks, condiments, beverages, specialty_foods, spices, cooking_oils, baking_ingredients, gourmet_chocolates, artisanal_cheeses, organic_foods\n",
      "\n",
      "--- class_idx 1 | meat_poultry ---\n",
      "meat poultry. keywords: butcher, cuts, marination, grilling, roasting, seasoning, halal, organic, deli, marbling | path: grocery gourmet food\n",
      "\n",
      "--- class_idx 2 | jerky ---\n",
      "jerky. keywords: beef, turkey, chicken, venison, buffalo, kangaroo, elk, ostrich, bison, spicy | path: grocery gourmet food > meat poultry\n",
      "\n",
      "--- class_idx 3 | toys_games ---\n",
      "toys games. keywords: board_games, puzzles, action_figures, building_blocks, dolls, outdoor_toys, educational_toys, card_games, remote_control_toys, plush_toys\n",
      "\n",
      "--- class_idx 8 | beverages ---\n",
      "beverages. keywords: coffee, tea, energy_drinks, soft_drinks, bottled_water, juices, sports_drinks, smoothies, iced_tea, coconut_water | path: grocery gourmet food\n",
      "\n",
      "--- class_idx 169 | pet_supplies ---\n",
      "pet supplies. keywords: dog_food, cat_litter, pet_grooming_supplies, aquarium_accessories, bird_cages, reptile_habitats, small_animal_bedding, flea_and_tick_prevention, pet_dental_care, horse_grooming_tools\n",
      "\n",
      "--- class_idx 530 | breeding_tanks ---\n",
      "breeding tanks. keywords: breeding_tanks, fish_breeding, spawning_tanks, fry_rearing, breeding_habitats, breeding_equipment, tank_dividers, breeding_accessories, egg_incubators, breeding_supplies | path: pet supplies > fish aquatic pets\n"
     ]
    }
   ],
   "source": [
    "label_df[\"text_len\"] = label_df[\"label_text\"].str.len()\n",
    "\n",
    "print(\"label_text len(min/mean/max):\",\n",
    "      int(label_df.text_len.min()),\n",
    "      float(label_df.text_len.mean()),\n",
    "      int(label_df.text_len.max()))\n",
    "\n",
    "print(\"keywords used(min/mean/max):\",\n",
    "      int(label_df.num_keywords_used.min()),\n",
    "      float(label_df.num_keywords_used.mean()),\n",
    "      int(label_df.num_keywords_used.max()))\n",
    "\n",
    "# 몇 개 눈으로 확인\n",
    "for ci in [0, 1, 2, 3, 8, 169, 530]:\n",
    "    print(\"\\n--- class_idx\", ci, \"|\", idx_to_name[ci], \"---\")\n",
    "    print(label_text[ci][:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42da61b9-6913-4e10-b2a7-211a5a9a2241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:20:17.923723Z",
     "iopub.status.busy": "2025-12-19T06:20:17.923420Z",
     "iopub.status.idle": "2025-12-19T06:20:17.935478Z",
     "shell.execute_reply": "2025-12-19T06:20:17.934866Z",
     "shell.execute_reply.started": "2025-12-19T06:20:17.923703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - artifacts/label_text.json\n",
      " - artifacts/label_text_table.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "ART.mkdir(exist_ok=True)\n",
    "\n",
    "with open(ART / \"label_text.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_text, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "label_df.to_csv(ART / \"label_text_table.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", ART / \"label_text.json\")\n",
    "print(\" -\", ART / \"label_text_table.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe556ce9-516c-4f87-a4e2-5c8acf837dd4",
   "metadata": {},
   "source": [
    "### Step3: Pseudo Label (Silver label)생성: \n",
    "##### unlabeled data인 train_df에 대해 TF-IDF 기반으로 각 문서에 2~3개 클래스 라벨 자동 부여 & 신뢰도 계산해 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2252d671-3f28-4c08-8fb3-37be14bdc617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:35:50.412938Z",
     "iopub.status.busy": "2025-12-19T06:35:50.412682Z",
     "iopub.status.idle": "2025-12-19T06:35:50.417962Z",
     "shell.execute_reply": "2025-12-19T06:35:50.417427Z",
     "shell.execute_reply.started": "2025-12-19T06:35:50.412918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step3 Config:\n",
      " TOPK_CAND: 30\n",
      " labels per doc: 2 ~ 3\n",
      " THRESH_THIRD: 0.55 | MARGIN_23: 0.05\n",
      " DOC_LIMIT: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Step2 산출물 확인\n",
    "assert \"label_text\" in globals(), \"Step2 label_text가 없습니다.\"\n",
    "assert \"label_df\" in globals(), \"Step2 label_df가 없습니다.\"\n",
    "assert \"train_df\" in globals(), \"Step1 train_df가 없습니다.\"\n",
    "assert \"classes_df\" in globals(), \"Step1 classes_df가 없습니다.\"\n",
    "assert \"edges\" in globals(), \"Step1 edges가 없습니다.\"\n",
    "\n",
    "# 설정\n",
    "TOPK_CAND = 30          # 후보 라벨 수 (처음엔 30 정도면 충분)\n",
    "FORCE_MIN_LABELS = 2\n",
    "FORCE_MAX_LABELS = 3\n",
    "\n",
    "# 3개 라벨을 줄지 결정하는 규칙 파라미터\n",
    "THRESH_THIRD = 0.55     # top3 점수가 이 이상이면 3개 허용 (TFIDF cosine 기준, 데이터에 따라 조정)\n",
    "MARGIN_23 = 0.05        # top2 - top3 차이가 작으면(비슷하면) 3개 허용\n",
    "\n",
    "# 샘플링(속도 검증용) : 처음엔 전체 실행하되, 문제 생기면 숫자 줄여 디버그\n",
    "DOC_LIMIT = None        # 예: 5000으로 줄여 시험 가능. 전체면 None\n",
    "\n",
    "print(\"Step3 Config:\")\n",
    "print(\" TOPK_CAND:\", TOPK_CAND)\n",
    "print(\" labels per doc:\", FORCE_MIN_LABELS, \"~\", FORCE_MAX_LABELS)\n",
    "print(\" THRESH_THIRD:\", THRESH_THIRD, \"| MARGIN_23:\", MARGIN_23)\n",
    "print(\" DOC_LIMIT:\", DOC_LIMIT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "599e6baa-65e7-417e-acfd-b1fe91ba35a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:35:51.345027Z",
     "iopub.status.busy": "2025-12-19T06:35:51.344810Z",
     "iopub.status.idle": "2025-12-19T06:35:55.464985Z",
     "shell.execute_reply": "2025-12-19T06:35:55.464400Z",
     "shell.execute_reply.started": "2025-12-19T06:35:51.345007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF shapes:\n",
      " X_doc: (29487, 200000)\n",
      " X_label: (531, 200000)\n",
      " vocab size: 200000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 문서 준비\n",
    "docs = train_df[\"text\"].astype(str).tolist()\n",
    "doc_ids = train_df[\"doc_id\"].astype(str).tolist()\n",
    "\n",
    "if DOC_LIMIT is not None:\n",
    "    docs = docs[:DOC_LIMIT]\n",
    "    doc_ids = doc_ids[:DOC_LIMIT]\n",
    "\n",
    "# 라벨 텍스트 준비 (531개 고정)\n",
    "label_texts = [label_text[i] for i in range(len(classes_df))]\n",
    "\n",
    "# TF-IDF 설정(보수적)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    max_features=200000,\n",
    ")\n",
    "\n",
    "X_doc = vectorizer.fit_transform(docs)          # (N_docs, V)\n",
    "X_label = vectorizer.transform(label_texts)     # (531, V)\n",
    "\n",
    "# cosine 유사도 위해 L2 normalize\n",
    "X_doc = normalize(X_doc)\n",
    "X_label = normalize(X_label)\n",
    "\n",
    "print(\"TFIDF shapes:\")\n",
    "print(\" X_doc:\", X_doc.shape)\n",
    "print(\" X_label:\", X_label.shape)\n",
    "print(\" vocab size:\", len(vectorizer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67570701-2c45-424a-92d9-7354e30d44df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:49:06.889125Z",
     "iopub.status.busy": "2025-12-19T06:49:06.888855Z",
     "iopub.status.idle": "2025-12-19T06:49:07.052930Z",
     "shell.execute_reply": "2025-12-19T06:49:07.052215Z",
     "shell.execute_reply.started": "2025-12-19T06:49:06.889103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk debug:\n",
      " sim min/mean/max: 0.0 0.001785475945790756 0.26322913554963845\n",
      " top1 score sample: 0.08823033 top3: 0.059450895 top10: 0.04230768\n",
      "Done. topk_idx/topk_score shapes: (29487, 30) (29487, 30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = X_doc.shape[0]\n",
    "C = X_label.shape[0]\n",
    "\n",
    "# 배치 계산: N x 531 전체를 한 번에 만들면 메모리는 버틸 수 있으나,\n",
    "# 안전하게 chunk로 topk만 뽑습니다.\n",
    "CHUNK = 2000\n",
    "\n",
    "topk_idx = np.zeros((N, TOPK_CAND), dtype=np.int32)\n",
    "topk_score = np.zeros((N, TOPK_CAND), dtype=np.float32)\n",
    "\n",
    "for start in range(0, N, CHUNK):\n",
    "    end = min(N, start + CHUNK)\n",
    "    # (chunk, V) dot (V, C) -> (chunk, C)\n",
    "    sim = X_doc[start:end].dot(X_label.T).toarray()  # cosine similarity (0~1+)\n",
    "    # topk\n",
    "    idx_part = np.argpartition(-sim, TOPK_CAND-1, axis=1)[:, :TOPK_CAND]\n",
    "    score_part = np.take_along_axis(sim, idx_part, axis=1)\n",
    "\n",
    "    # sort within topk\n",
    "    order = np.argsort(-score_part, axis=1)\n",
    "    idx_sorted = np.take_along_axis(idx_part, order, axis=1)\n",
    "    score_sorted = np.take_along_axis(score_part, order, axis=1)\n",
    "\n",
    "    topk_idx[start:end] = idx_sorted.astype(np.int32)\n",
    "    topk_score[start:end] = score_sorted.astype(np.float32)\n",
    "\n",
    "    if start == 0:\n",
    "        print(\"First chunk debug:\")\n",
    "        print(\" sim min/mean/max:\", float(sim.min()), float(sim.mean()), float(sim.max()))\n",
    "        print(\" top1 score sample:\", topk_score[0,0], \"top3:\", topk_score[0,2], \"top10:\", topk_score[0,9])\n",
    "\n",
    "print(\"Done. topk_idx/topk_score shapes:\", topk_idx.shape, topk_score.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9108bbe7-c382-4785-87ae-7aac67709d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:51:07.058482Z",
     "iopub.status.busy": "2025-12-19T06:51:07.058178Z",
     "iopub.status.idle": "2025-12-19T06:51:07.254875Z",
     "shell.execute_reply": "2025-12-19T06:51:07.254276Z",
     "shell.execute_reply.started": "2025-12-19T06:51:07.058459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels per doc distribution:\n",
      "2    12764\n",
      "3    16723\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "confidence describe:\n",
      "count    29487.000000\n",
      "mean         0.052508\n",
      "std          0.034802\n",
      "min          0.000000\n",
      "25%          0.029144\n",
      "50%          0.045683\n",
      "75%          0.068086\n",
      "max          0.389068\n",
      "Name: confidence, dtype: float64\n",
      "\n",
      "Top 15 frequent labels:\n",
      " 90 hair_color                     -> 1543\n",
      " 30 electronics_for_kids           -> 1506\n",
      "104 electronic_pets                -> 1451\n",
      "350 sports_supplements             -> 1001\n",
      " 15 action_toy_figures             -> 974\n",
      "461 hair_relaxers                  -> 971\n",
      "346 milk                           -> 961\n",
      "473 hair_perms_texturizers         -> 914\n",
      "220 fragrance                      -> 863\n",
      "344 chocolate_truffles             -> 698\n",
      " 40 baby_products                  -> 698\n",
      "379 toy_gift_sets                  -> 651\n",
      "435 game_room_games                -> 650\n",
      "166 accessories                    -> 613\n",
      "177 cameras_camcorders             -> 612\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>confidence</th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>top3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[137, 87]</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.088230</td>\n",
       "      <td>0.072477</td>\n",
       "      <td>0.059451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[266, 276, 490]</td>\n",
       "      <td>0.054418</td>\n",
       "      <td>0.053876</td>\n",
       "      <td>0.053649</td>\n",
       "      <td>0.051482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[409, 381]</td>\n",
       "      <td>0.057090</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.053065</td>\n",
       "      <td>0.045784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[90, 461, 473]</td>\n",
       "      <td>0.065674</td>\n",
       "      <td>0.066375</td>\n",
       "      <td>0.063531</td>\n",
       "      <td>0.063531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[308, 376, 449]</td>\n",
       "      <td>0.025899</td>\n",
       "      <td>0.028305</td>\n",
       "      <td>0.022843</td>\n",
       "      <td>0.022631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[43, 173, 145]</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.031117</td>\n",
       "      <td>0.009747</td>\n",
       "      <td>0.009734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[158, 210]</td>\n",
       "      <td>0.056686</td>\n",
       "      <td>0.066117</td>\n",
       "      <td>0.041233</td>\n",
       "      <td>0.029989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[350, 346, 344]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id           labels  confidence      top1      top2      top3\n",
       "0      0        [137, 87]    0.084811  0.088230  0.072477  0.059451\n",
       "1      1  [266, 276, 490]    0.054418  0.053876  0.053649  0.051482\n",
       "2      2       [409, 381]    0.057090  0.055450  0.053065  0.045784\n",
       "3      3   [90, 461, 473]    0.065674  0.066375  0.063531  0.063531\n",
       "4      4  [308, 376, 449]    0.025899  0.028305  0.022843  0.022631\n",
       "5      5   [43, 173, 145]    0.020517  0.031117  0.009747  0.009734\n",
       "6      6       [158, 210]    0.056686  0.066117  0.041233  0.029989\n",
       "7      7  [350, 346, 344]    0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[High-Conf] percentile=60 -> threshold=0.053064\n",
      "high-conf count: 11795 / 29487\n",
      "\n",
      "labels per doc distribution (high-conf subset):\n",
      "2    6208\n",
      "3    5587\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "Top 10 frequent labels (high-conf subset):\n",
      " 90 hair_color                     -> 666\n",
      "461 hair_relaxers                  -> 473\n",
      "473 hair_perms_texturizers         -> 459\n",
      "220 fragrance                      -> 452\n",
      " 40 baby_products                  -> 373\n",
      "435 game_room_games                -> 355\n",
      "  8 beverages                      -> 327\n",
      "127 car_seat_stroller_toys         -> 292\n",
      "294 game_collections               -> 286\n",
      "142 cloth_diapers                  -> 280\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "\n",
    "# 튜닝값 확정\n",
    "THRESH_THIRD = 0.06866627186536789\n",
    "MARGIN_23    = 0.003\n",
    "\n",
    "def select_labels(scores, idxs, min_k=2, max_k=3,\n",
    "                  thresh_third=0.0687, margin_23=0.003):\n",
    "    chosen = [int(idxs[0]), int(idxs[1])]\n",
    "    s2 = float(scores[1])\n",
    "    s3 = float(scores[2])\n",
    "\n",
    "    if max_k >= 3:\n",
    "        if (s3 >= thresh_third) or ((s2 - s3) <= margin_23):\n",
    "            chosen.append(int(idxs[2]))\n",
    "\n",
    "    if len(chosen) < min_k:\n",
    "        chosen = list(map(int, idxs[:min_k]))\n",
    "    return chosen[:max_k]\n",
    "\n",
    "N = topk_idx.shape[0]\n",
    "silver_labels = []\n",
    "conf_list = []\n",
    "\n",
    "top1 = topk_score[:,0].astype(float)\n",
    "top2 = topk_score[:,1].astype(float)\n",
    "top3 = topk_score[:,2].astype(float)\n",
    "top4 = topk_score[:,3].astype(float)\n",
    "\n",
    "for i in range(N):\n",
    "    labs = select_labels(topk_score[i], topk_idx[i],\n",
    "                         min_k=2, max_k=3,\n",
    "                         thresh_third=THRESH_THIRD,\n",
    "                         margin_23=MARGIN_23)\n",
    "    silver_labels.append(labs)\n",
    "\n",
    "    # confidence(간단): top2 평균 + 0.25*(top2-top4) 양수부분\n",
    "    conf = (top1[i] + top2[i]) / 2.0 + 0.25 * max(0.0, (top2[i] - top4[i]))\n",
    "    conf_list.append(float(conf))\n",
    "\n",
    "silver_df = pd.DataFrame({\n",
    "    \"doc_id\": train_df[\"doc_id\"].astype(str).tolist()[:N],\n",
    "    \"labels\": silver_labels,\n",
    "    \"confidence\": conf_list,\n",
    "    \"top1\": top1,\n",
    "    \"top2\": top2,\n",
    "    \"top3\": top3,\n",
    "})\n",
    "\n",
    "# (1) 2개 vs 3개 분포\n",
    "print(\"labels per doc distribution:\")\n",
    "print(silver_df[\"labels\"].apply(len).value_counts().sort_index())\n",
    "\n",
    "# (2) confidence 통계\n",
    "print(\"\\nconfidence describe:\")\n",
    "print(silver_df[\"confidence\"].describe())\n",
    "\n",
    "# (3) 상위 라벨 빈도(쏠림 체크)\n",
    "cnt = Counter()\n",
    "for labs in silver_labels:\n",
    "    cnt.update(labs)\n",
    "\n",
    "top15 = cnt.most_common(15)\n",
    "print(\"\\nTop 15 frequent labels:\")\n",
    "for ci, ncnt in top15:\n",
    "    cname = classes_df.loc[classes_df.class_idx==ci, \"class_name\"].iloc[0]\n",
    "    print(f\"{ci:3d} {cname:30s} -> {ncnt}\")\n",
    "\n",
    "display(silver_df.head(8))\n",
    "\n",
    "# high-confidence threshold를 퍼센타일로 결정\n",
    "# 추천: 상위 60%만 학습에 사용(노이즈 감소). 필요 시 70%까지도 가능\n",
    "CONF_PCTL = 60\n",
    "conf_thr = float(np.percentile(silver_df[\"confidence\"].values, CONF_PCTL))\n",
    "\n",
    "silver_df[\"is_high_conf\"] = (silver_df[\"confidence\"] >= conf_thr)\n",
    "\n",
    "print(f\"\\n[High-Conf] percentile={CONF_PCTL} -> threshold={conf_thr:.6f}\")\n",
    "print(\"high-conf count:\", int(silver_df[\"is_high_conf\"].sum()), \"/\", len(silver_df))\n",
    "\n",
    "# high-conf subset의 2/3 라벨 분포도 확인\n",
    "print(\"\\nlabels per doc distribution (high-conf subset):\")\n",
    "print(silver_df.loc[silver_df.is_high_conf, \"labels\"].apply(len).value_counts().sort_index())\n",
    "\n",
    "# high-conf subset에서 상위 라벨 쏠림 체크\n",
    "from collections import Counter\n",
    "cnt_h = Counter()\n",
    "for labs in silver_df.loc[silver_df.is_high_conf, \"labels\"]:\n",
    "    cnt_h.update(labs)\n",
    "\n",
    "print(\"\\nTop 10 frequent labels (high-conf subset):\")\n",
    "for ci, ncnt in cnt_h.most_common(10):\n",
    "    cname = classes_df.loc[classes_df.class_idx==ci, \"class_name\"].iloc[0]\n",
    "    print(f\"{ci:3d} {cname:30s} -> {ncnt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16c278ed-f6f6-460b-a238-79762865d322",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T06:51:15.404394Z",
     "iopub.status.busy": "2025-12-19T06:51:15.404129Z",
     "iopub.status.idle": "2025-12-19T06:51:15.485324Z",
     "shell.execute_reply": "2025-12-19T06:51:15.484802Z",
     "shell.execute_reply.started": "2025-12-19T06:51:15.404374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - artifacts/silver_train_tfidf_all.jsonl\n",
      " - artifacts/silver_train_tfidf_highconf.jsonl\n",
      " - artifacts/silver_train_tfidf_meta.json\n",
      "\n",
      "High-conf preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>confidence</th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>top3</th>\n",
       "      <th>is_high_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[137, 87]</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.088230</td>\n",
       "      <td>0.072477</td>\n",
       "      <td>0.059451</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[266, 276, 490]</td>\n",
       "      <td>0.054418</td>\n",
       "      <td>0.053876</td>\n",
       "      <td>0.053649</td>\n",
       "      <td>0.051482</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[409, 381]</td>\n",
       "      <td>0.057090</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.053065</td>\n",
       "      <td>0.045784</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[90, 461, 473]</td>\n",
       "      <td>0.065674</td>\n",
       "      <td>0.066375</td>\n",
       "      <td>0.063531</td>\n",
       "      <td>0.063531</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[158, 210]</td>\n",
       "      <td>0.056686</td>\n",
       "      <td>0.066117</td>\n",
       "      <td>0.041233</td>\n",
       "      <td>0.029989</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[129, 195, 130]</td>\n",
       "      <td>0.194676</td>\n",
       "      <td>0.208124</td>\n",
       "      <td>0.154710</td>\n",
       "      <td>0.102777</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>[524, 448, 64]</td>\n",
       "      <td>0.100061</td>\n",
       "      <td>0.103397</td>\n",
       "      <td>0.089236</td>\n",
       "      <td>0.078065</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>[435, 294]</td>\n",
       "      <td>0.074757</td>\n",
       "      <td>0.071439</td>\n",
       "      <td>0.065224</td>\n",
       "      <td>0.061182</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id           labels  confidence      top1      top2      top3  \\\n",
       "0       0        [137, 87]    0.084811  0.088230  0.072477  0.059451   \n",
       "1       1  [266, 276, 490]    0.054418  0.053876  0.053649  0.051482   \n",
       "2       2       [409, 381]    0.057090  0.055450  0.053065  0.045784   \n",
       "3       3   [90, 461, 473]    0.065674  0.066375  0.063531  0.063531   \n",
       "6       6       [158, 210]    0.056686  0.066117  0.041233  0.029989   \n",
       "8       8  [129, 195, 130]    0.194676  0.208124  0.154710  0.102777   \n",
       "17     17   [524, 448, 64]    0.100061  0.103397  0.089236  0.078065   \n",
       "23     23       [435, 294]    0.074757  0.071439  0.065224  0.061182   \n",
       "\n",
       "    is_high_conf  \n",
       "0           True  \n",
       "1           True  \n",
       "2           True  \n",
       "3           True  \n",
       "6           True  \n",
       "8           True  \n",
       "17          True  \n",
       "23          True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "ART.mkdir(exist_ok=True)\n",
    "\n",
    "# 1) 전체 저장\n",
    "out_all = ART / \"silver_train_tfidf_all.jsonl\"\n",
    "silver_df.to_json(out_all, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "# 2) high-confidence subset 저장\n",
    "out_hc = ART / \"silver_train_tfidf_highconf.jsonl\"\n",
    "silver_high = silver_df[silver_df[\"is_high_conf\"]].copy()\n",
    "silver_high.to_json(out_hc, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "# 3) 메타 저장(재현성)\n",
    "meta = {\n",
    "    \"method\": \"tfidf_label_text_cosine\",\n",
    "    \"TOPK_CAND\": TOPK_CAND,\n",
    "    \"labels_per_doc\": [FORCE_MIN_LABELS, FORCE_MAX_LABELS],\n",
    "    \"THRESH_THIRD\": THRESH_THIRD,\n",
    "    \"MARGIN_23\": MARGIN_23,\n",
    "    \"DOC_LIMIT\": DOC_LIMIT,\n",
    "    \"vectorizer\": {\n",
    "        \"ngram_range\": (1,2),\n",
    "        \"min_df\": 2,\n",
    "        \"max_df\": 0.9,\n",
    "        \"max_features\": 200000,\n",
    "    },\n",
    "    \"high_conf\": {\n",
    "        \"percentile\": int(CONF_PCTL),\n",
    "        \"threshold\": float(conf_thr),\n",
    "        \"count\": int(len(silver_high)),\n",
    "        \"ratio\": float(len(silver_high) / len(silver_df)),\n",
    "    },\n",
    "    \"num_docs\": int(len(silver_df)),\n",
    "    \"num_classes\": int(len(classes_df)),\n",
    "}\n",
    "with open(ART / \"silver_train_tfidf_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", out_all)\n",
    "print(\" -\", out_hc)\n",
    "print(\" -\", ART / \"silver_train_tfidf_meta.json\")\n",
    "\n",
    "print(\"\\nHigh-conf preview:\")\n",
    "display(silver_high.head(8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51f3c4-4f8c-47da-aec3-6058cb20eed5",
   "metadata": {},
   "source": [
    "#### Step4 : Text classification model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09e7e878-5b3e-4719-8ca1-e6cabba71929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:08:12.556745Z",
     "iopub.status.busy": "2025-12-19T07:08:12.556478Z",
     "iopub.status.idle": "2025-12-19T07:08:12.624551Z",
     "shell.execute_reply": "2025-12-19T07:08:12.623962Z",
     "shell.execute_reply.started": "2025-12-19T07:08:12.556725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver(highconf): (11795, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>confidence</th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>top3</th>\n",
       "      <th>is_high_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[137, 87]</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.088230</td>\n",
       "      <td>0.072477</td>\n",
       "      <td>0.059451</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[266, 276, 490]</td>\n",
       "      <td>0.054418</td>\n",
       "      <td>0.053876</td>\n",
       "      <td>0.053649</td>\n",
       "      <td>0.051482</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[409, 381]</td>\n",
       "      <td>0.057090</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.053065</td>\n",
       "      <td>0.045784</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id           labels  confidence      top1      top2      top3  \\\n",
       "0       0        [137, 87]    0.084811  0.088230  0.072477  0.059451   \n",
       "1       1  [266, 276, 490]    0.054418  0.053876  0.053649  0.051482   \n",
       "2       2       [409, 381]    0.057090  0.055450  0.053065  0.045784   \n",
       "\n",
       "   is_high_conf  \n",
       "0          True  \n",
       "1          True  \n",
       "2          True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged data: (11795, 8) | missing_text: 0\n",
      "labels per doc (highconf):\n",
      "2    6208\n",
      "3    5587\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "silver_path = ART / \"silver_train_tfidf_highconf.jsonl\"\n",
    "assert silver_path.exists(), f\"Missing: {silver_path}\"\n",
    "\n",
    "silver = pd.read_json(silver_path, lines=True)\n",
    "print(\"silver(highconf):\", silver.shape)\n",
    "display(silver.head(3))\n",
    "\n",
    "# train_df에서 doc_id로 text join\n",
    "# (train_df는 Step1에서 이미 로드되어 있어야 함)\n",
    "assert \"train_df\" in globals(), \"train_df not found\"\n",
    "train_map = train_df[[\"doc_id\",\"text\"]].copy()\n",
    "train_map[\"doc_id\"] = train_map[\"doc_id\"].astype(str)\n",
    "\n",
    "silver[\"doc_id\"] = silver[\"doc_id\"].astype(str)\n",
    "data = silver.merge(train_map, on=\"doc_id\", how=\"left\")\n",
    "\n",
    "missing_text = data[\"text\"].isna().sum()\n",
    "print(\"merged data:\", data.shape, \"| missing_text:\", int(missing_text))\n",
    "if missing_text > 0:\n",
    "    display(data[data[\"text\"].isna()].head(5))\n",
    "    raise ValueError(\"Some doc_id not found in train_df\")\n",
    "\n",
    "# 학습용 컬럼\n",
    "X_text = data[\"text\"].astype(str).tolist()\n",
    "Y_labels = data[\"labels\"].tolist()   # list of list[int]\n",
    "conf = data[\"confidence\"].astype(float).values\n",
    "\n",
    "# 라벨 수 분포 확인\n",
    "cnt = pd.Series([len(x) for x in Y_labels]).value_counts().sort_index()\n",
    "print(\"labels per doc (highconf):\")\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f3f7516-7dd7-47c7-8f4a-f59e688137e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:08:24.908190Z",
     "iopub.status.busy": "2025-12-19T07:08:24.907918Z",
     "iopub.status.idle": "2025-12-19T07:08:26.555136Z",
     "shell.execute_reply": "2025-12-19T07:08:26.554571Z",
     "shell.execute_reply.started": "2025-12-19T07:08:24.908169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: 10615 1180\n",
      "TFIDF shapes: (10615, 55968) (1180, 55968) | vocab: 55968\n",
      "Y bin shapes: (10615, 531) (1180, 531) | classes: 531\n",
      "train label density (avg #labels): 2.4740461610927933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "SEED = 42\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "idx = np.arange(len(X_text))\n",
    "tr_idx, va_idx = train_test_split(idx, test_size=VAL_RATIO, random_state=SEED, shuffle=True)\n",
    "\n",
    "X_tr = [X_text[i] for i in tr_idx]\n",
    "X_va = [X_text[i] for i in va_idx]\n",
    "Y_tr = [Y_labels[i] for i in tr_idx]\n",
    "Y_va = [Y_labels[i] for i in va_idx]\n",
    "\n",
    "print(\"split:\", len(X_tr), len(X_va))\n",
    "\n",
    "# TF-IDF (Step3보다 살짝 보수적으로 희귀 토큰 영향 줄임)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    max_features=200000,\n",
    ")\n",
    "\n",
    "Xtr_vec = vectorizer.fit_transform(X_tr)\n",
    "Xva_vec = vectorizer.transform(X_va)\n",
    "\n",
    "print(\"TFIDF shapes:\", Xtr_vec.shape, Xva_vec.shape, \"| vocab:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "# MultiLabel -> multi-hot (531 classes)\n",
    "NUM_CLASSES = len(classes_df) if \"classes_df\" in globals() else 531\n",
    "mlb = MultiLabelBinarizer(classes=list(range(NUM_CLASSES)))\n",
    "Ytr_bin = mlb.fit_transform(Y_tr)\n",
    "Yva_bin = mlb.transform(Y_va)\n",
    "\n",
    "print(\"Y bin shapes:\", Ytr_bin.shape, Yva_bin.shape, \"| classes:\", len(mlb.classes_))\n",
    "print(\"train label density (avg #labels):\", Ytr_bin.sum(axis=1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb164a21-57e8-4295-80a1-015d3b07a540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:08:45.327953Z",
     "iopub.status.busy": "2025-12-19T07:08:45.327658Z",
     "iopub.status.idle": "2025-12-19T07:08:48.897362Z",
     "shell.execute_reply": "2025-12-19T07:08:48.896703Z",
     "shell.execute_reply.started": "2025-12-19T07:08:45.327929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/esci/lib/python3.9/site-packages/sklearn/multiclass.py:77: UserWarning: Label not 383 is present in all training examples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/esci/lib/python3.9/site-packages/sklearn/multiclass.py:77: UserWarning: Label not 439 is present in all training examples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/esci/lib/python3.9/site-packages/sklearn/multiclass.py:77: UserWarning: Label not 505 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# LinearSVC는 대규모 sparse 텍스트에서 강한 베이스라인\n",
    "base_clf = LinearSVC(C=1.0)\n",
    "clf = OneVsRestClassifier(base_clf, n_jobs=-1)\n",
    "\n",
    "print(\"Training...\")\n",
    "clf.fit(Xtr_vec, Ytr_bin)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "268f9419-415a-42cd-9451-c7525c2c2960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:12:38.407712Z",
     "iopub.status.busy": "2025-12-19T07:12:38.407443Z",
     "iopub.status.idle": "2025-12-19T07:12:38.717340Z",
     "shell.execute_reply": "2025-12-19T07:12:38.716822Z",
     "shell.execute_reply.started": "2025-12-19T07:12:38.407691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds (revised):\n",
      " THRESH_THIRD: 0.0009821156265188567\n",
      " MARGIN_23   : 0.13019305185127453\n",
      "gap23 percentiles: [0.0, 0.13019305185127453, 0.41966545089410423, 0.6925465777002563]\n",
      "Val micro-F1  : 0.43772175536881425\n",
      "Val sample-F1 : 0.42819209039548023\n",
      "\n",
      "Pred labels per doc distribution:\n",
      "2    1100\n",
      "3      80\n",
      "dtype: int64\n",
      "\n",
      "[ex 0] GT=[137, 241] | PR=[241, 137]\n",
      "master massage monroe salon size portable massage table , 30 inch i purchased this massage table primarily for home use ( we have a wonderful massage therapist who does out calls ) and this makes it e\n",
      "\n",
      "[ex 1] GT=[271, 366] | PR=[383, 505]\n",
      "popcorn nut salt 24 oz i have n't ordered this but i was reading some bad reviews saying it was just salt , not flavored , which is true . i also read that it is finer than regular store bought salt a\n",
      "\n",
      "[ex 2] GT=[127, 152] | PR=[127, 152]\n",
      "amazon . com somehow i lost my first go go babyz and had to order this one because i will not travel without it . it takes a little patience to fasten the carseat to it , but as long as you know how t\n",
      "\n",
      "[ex 3] GT=[111, 125, 441] | PR=[439, 383]\n",
      "bounce fabric softener sheets , outdoor fresh scent , 120 count box ( pack of 2 ) ca n't beat the price ! item arrived on time in good condition warped in extra plastic packing . beats driving to the \n",
      "\n",
      "[ex 4] GT=[395, 42] | PR=[395, 42]\n",
      "baby magic calming baby lotion w lavender chamomile i love baby magic and it is getting harder to find . i like the way it smells better than johnson and johnson 's and when i have babies with more se\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "scores = clf.decision_function(Xva_vec)\n",
    "scores = np.asarray(scores)\n",
    "\n",
    "# top score들\n",
    "idx_sorted = np.argsort(-scores, axis=1)\n",
    "top1_idx = idx_sorted[:, 0]\n",
    "top2_idx = idx_sorted[:, 1]\n",
    "top3_idx = idx_sorted[:, 2]\n",
    "\n",
    "top1 = scores[np.arange(scores.shape[0]), top1_idx]\n",
    "top2 = scores[np.arange(scores.shape[0]), top2_idx]\n",
    "top3 = scores[np.arange(scores.shape[0]), top3_idx]\n",
    "\n",
    "gap23 = top2 - top3\n",
    "\n",
    "# 더 안정적인 임계치(보수적으로)\n",
    "THRESH_THIRD = float(np.percentile(top3, 85))   # 그대로(상위 10%)\n",
    "MARGIN_23 = float(np.percentile(gap23, 75))     # 0 방지 + 더 보수적\n",
    "\n",
    "# 안전장치: MARGIN이 너무 작으면 최소값 부여\n",
    "MARGIN_23 = max(MARGIN_23, 0.05)\n",
    "\n",
    "print(\"thresholds (revised):\")\n",
    "print(\" THRESH_THIRD:\", THRESH_THIRD)\n",
    "print(\" MARGIN_23   :\", MARGIN_23)\n",
    "print(\"gap23 percentiles:\", np.percentile(gap23, [50, 75, 90, 95]).tolist())\n",
    "\n",
    "def predict_2or3(score_row, thresh_third, margin_23):\n",
    "    idx = np.argsort(-score_row)[:3]\n",
    "    s1, s2, s3 = float(score_row[idx[0]]), float(score_row[idx[1]]), float(score_row[idx[2]])\n",
    "    labs = [int(idx[0]), int(idx[1])]\n",
    "    # 3라벨은 더 엄격하게:\n",
    "    # - top3가 충분히 크고\n",
    "    # - top2-top3 차이가 충분히 작을 때만\n",
    "    if (s3 >= thresh_third) and ((s2 - s3) <= margin_23):\n",
    "        labs.append(int(idx[2]))\n",
    "    return labs\n",
    "\n",
    "pred_labels = [predict_2or3(scores[i], THRESH_THIRD, MARGIN_23) for i in range(scores.shape[0])]\n",
    "\n",
    "Ypred_bin = np.zeros_like(Yva_bin)\n",
    "for i, labs in enumerate(pred_labels):\n",
    "    Ypred_bin[i, labs] = 1\n",
    "\n",
    "micro = f1_score(Yva_bin, Ypred_bin, average=\"micro\", zero_division=0)\n",
    "samples = f1_score(Yva_bin, Ypred_bin, average=\"samples\", zero_division=0)\n",
    "print(\"Val micro-F1  :\", micro)\n",
    "print(\"Val sample-F1 :\", samples)\n",
    "\n",
    "dist = pd.Series([len(x) for x in pred_labels]).value_counts().sort_index()\n",
    "print(\"\\nPred labels per doc distribution:\")\n",
    "print(dist)\n",
    "\n",
    "# 예시 5개\n",
    "for j in range(5):\n",
    "    gt = Y_va[j]\n",
    "    pr = pred_labels[j]\n",
    "    print(f\"\\n[ex {j}] GT={gt} | PR={pr}\")\n",
    "    print(X_va[j][:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7addba68-b478-440d-971a-dcc4de156cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:14:04.321927Z",
     "iopub.status.busy": "2025-12-19T07:14:04.321659Z",
     "iopub.status.idle": "2025-12-19T07:14:06.828525Z",
     "shell.execute_reply": "2025-12-19T07:14:06.827445Z",
     "shell.execute_reply.started": "2025-12-19T07:14:04.321906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - artifacts/step4_tfidf_linearSVC_ovr.joblib\n",
      " - artifacts/step4_model_meta.json\n"
     ]
    }
   ],
   "source": [
    "import joblib, json\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "ART.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = ART / \"step4_tfidf_linearSVC_ovr.joblib\"\n",
    "joblib.dump(\n",
    "    {\"vectorizer\": vectorizer, \"model\": clf, \"mlb\": mlb,\n",
    "     \"thresholds\": {\"THRESH_THIRD\": THRESH_THIRD, \"MARGIN_23\": MARGIN_23}},\n",
    "    model_path\n",
    ")\n",
    "\n",
    "meta = {\n",
    "    \"model\": \"OneVsRest(LinearSVC)\",\n",
    "    \"tfidf\": {\"ngram_range\": (1,2), \"min_df\": 3, \"max_df\": 0.9, \"max_features\": 200000},\n",
    "    \"val_ratio\": VAL_RATIO,\n",
    "    \"seed\": SEED,\n",
    "    \"thresholds\": {\"THRESH_THIRD\": float(THRESH_THIRD), \"MARGIN_23\": float(MARGIN_23)},\n",
    "    \"val_micro_f1\": float(micro),\n",
    "    \"val_sample_f1\": float(samples),\n",
    "    \"pred_label_dist\": {str(k): int(v) for k,v in pd.Series([len(x) for x in pred_labels]).value_counts().sort_index().items()},\n",
    "    \"train_docs\": int(len(X_tr)),\n",
    "    \"val_docs\": int(len(X_va)),\n",
    "}\n",
    "with open(ART / \"step4_model_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", model_path)\n",
    "print(\" -\", ART / \"step4_model_meta.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45432b9e-150e-429c-b98b-1886c61ca8db",
   "metadata": {},
   "source": [
    "### Step5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba81d120-5bf7-427c-a185-b601639e783e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:19:09.288826Z",
     "iopub.status.busy": "2025-12-19T07:19:09.288526Z",
     "iopub.status.idle": "2025-12-19T07:19:17.419166Z",
     "shell.execute_reply": "2025-12-19T07:19:17.418503Z",
     "shell.execute_reply.started": "2025-12-19T07:19:09.288806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF: (10615, 55968) (1180, 55968) | vocab: 55968\n",
      "Doc embedding: (10615, 256) (1180, 256)\n",
      "Class init feat: (531, 256)\n",
      "Y bin: (10615, 531) (1180, 531) | avg labels(train): 2.4740462\n",
      "Graph: nodes 531 | undirected edges (nonzero offdiag): 1136\n",
      "A_hat stats: 0.0 0.001591013977304101 0.4999999701976776\n"
     ]
    }
   ],
   "source": [
    "import random, numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 재현성\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Step3 highconf 로드 (학습 신뢰도 확보)\n",
    "ART = Path(\"artifacts\")\n",
    "silver_path = ART / \"silver_train_tfidf_highconf.jsonl\"\n",
    "silver = pd.read_json(silver_path, lines=True)\n",
    "\n",
    "# train_df에서 text 조인\n",
    "train_map = train_df[[\"doc_id\",\"text\"]].copy()\n",
    "train_map[\"doc_id\"] = train_map[\"doc_id\"].astype(str)\n",
    "silver[\"doc_id\"] = silver[\"doc_id\"].astype(str)\n",
    "data = silver.merge(train_map, on=\"doc_id\", how=\"left\")\n",
    "assert data[\"text\"].isna().sum() == 0\n",
    "\n",
    "X_text = data[\"text\"].astype(str).tolist()\n",
    "Y_labels = data[\"labels\"].tolist()\n",
    "\n",
    "# split (Step4와 동일하게 10% val)\n",
    "VAL_RATIO = 0.1\n",
    "idx = np.arange(len(X_text))\n",
    "tr_idx, va_idx = train_test_split(idx, test_size=VAL_RATIO, random_state=SEED, shuffle=True)\n",
    "\n",
    "X_tr = [X_text[i] for i in tr_idx]\n",
    "X_va = [X_text[i] for i in va_idx]\n",
    "Y_tr = [Y_labels[i] for i in tr_idx]\n",
    "Y_va = [Y_labels[i] for i in va_idx]\n",
    "\n",
    "NUM_CLASSES = len(classes_df)\n",
    "\n",
    "# TF-IDF (Step4와 비슷한 설정)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True, ngram_range=(1,2),\n",
    "    min_df=3, max_df=0.9, max_features=200000\n",
    ")\n",
    "Xtr_tfidf = vectorizer.fit_transform(X_tr)\n",
    "Xva_tfidf = vectorizer.transform(X_va)\n",
    "\n",
    "print(\"TFIDF:\", Xtr_tfidf.shape, Xva_tfidf.shape, \"| vocab:\", len(vectorizer.vocabulary_))\n",
    "\n",
    "# SVD로 밀집 문서 임베딩 만들기 (GCN과 내적하기 위해)\n",
    "EMB_DIM = 256\n",
    "svd = TruncatedSVD(n_components=EMB_DIM, random_state=SEED)\n",
    "Z_tr = svd.fit_transform(Xtr_tfidf)   # (Ntr, 256)\n",
    "Z_va = svd.transform(Xva_tfidf)       # (Nva, 256)\n",
    "\n",
    "print(\"Doc embedding:\", Z_tr.shape, Z_va.shape)\n",
    "\n",
    "# 클래스 초기 특징: label_text를 같은 vectorizer로 -> 같은 svd 공간으로\n",
    "label_texts = [label_text[i] for i in range(NUM_CLASSES)]\n",
    "Xc_tfidf = vectorizer.transform(label_texts)\n",
    "Xc0 = svd.transform(Xc_tfidf)         # (531, 256)\n",
    "print(\"Class init feat:\", Xc0.shape)\n",
    "\n",
    "# 멀티라벨 binarize\n",
    "mlb = MultiLabelBinarizer(classes=list(range(NUM_CLASSES)))\n",
    "Ytr_bin = mlb.fit_transform(Y_tr).astype(np.float32)\n",
    "Yva_bin = mlb.transform(Y_va).astype(np.float32)\n",
    "\n",
    "print(\"Y bin:\", Ytr_bin.shape, Yva_bin.shape, \"| avg labels(train):\", Ytr_bin.sum(axis=1).mean())\n",
    "\n",
    "# ====== taxonomy 그래프 -> 정규화 adjacency(A_hat) ======\n",
    "# edges: list[(parent, child)] in Step1\n",
    "N = NUM_CLASSES\n",
    "A = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "# 방향성을 약하게 만들기 위해(메시지 패싱 안정), undirected로 사용(부모-자식 모두 연결)\n",
    "for p, c in edges:\n",
    "    A[p, c] = 1.0\n",
    "    A[c, p] = 1.0\n",
    "\n",
    "# self-loop 추가\n",
    "A[np.arange(N), np.arange(N)] = 1.0\n",
    "\n",
    "deg = A.sum(axis=1)\n",
    "D_inv_sqrt = np.diag(1.0 / np.sqrt(np.maximum(deg, 1e-8)))\n",
    "A_hat = D_inv_sqrt @ A @ D_inv_sqrt   # (N,N)\n",
    "\n",
    "print(\"Graph:\", \"nodes\", N, \"| undirected edges (nonzero offdiag):\", int((A>0).sum() - N))\n",
    "print(\"A_hat stats:\", float(A_hat.min()), float(A_hat.mean()), float(A_hat.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a9c66bc-157a-484b-959b-7038058e0c7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:23:34.687138Z",
     "iopub.status.busy": "2025-12-19T07:23:34.686876Z",
     "iopub.status.idle": "2025-12-19T07:23:35.802898Z",
     "shell.execute_reply": "2025-12-19T07:23:35.802181Z",
     "shell.execute_reply.started": "2025-12-19T07:23:34.687119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_va: (1180, 531) min/mean/max: -2.535507356382762 -1.0667796914498313 2.752199370737678\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "bundle = joblib.load(ART / \"step4_tfidf_linearSVC_ovr.joblib\")\n",
    "vec4 = bundle[\"vectorizer\"]\n",
    "clf4 = bundle[\"model\"]\n",
    "mlb4 = bundle[\"mlb\"]\n",
    "\n",
    "# Step5의 train/val split을 그대로 쓰려면, 지금 Step5 Cell1에서 만든 X_va, Y_va, Yva_bin을 그대로 사용\n",
    "# (이미 존재한다고 가정)\n",
    "Xva_vec4 = vec4.transform(X_va)\n",
    "scores_va = clf4.decision_function(Xva_vec4)  # (Nva, 531)\n",
    "scores_va = np.asarray(scores_va)\n",
    "\n",
    "print(\"scores_va:\", scores_va.shape, \"min/mean/max:\", float(scores_va.min()), float(scores_va.mean()), float(scores_va.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ef8cefe-ceb6-4980-99bb-f6546e2469ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:26:34.987690Z",
     "iopub.status.busy": "2025-12-19T07:26:34.987183Z",
     "iopub.status.idle": "2025-12-19T07:26:35.004698Z",
     "shell.execute_reply": "2025-12-19T07:26:35.004053Z",
     "shell.execute_reply.started": "2025-12-19T07:26:34.987654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_pc_hat stats: 0.0 0.0018832391360774636 1.0\n",
      "top1 changed ratio: 0.023728813559322035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# edges: (parent, child)\n",
    "N = NUM_CLASSES\n",
    "\n",
    "A_pc = np.zeros((N, N), dtype=np.float32)  # parent->child\n",
    "for p, c in edges:\n",
    "    A_pc[c, p] = 1.0  # NOTE: (child row) <- parent col 로 두면 A_pc @ S 가 \"부모 점수의 자식 유입\"이 됨\n",
    "\n",
    "# self-loop(자기 보존)\n",
    "A_pc[np.arange(N), np.arange(N)] = 1.0\n",
    "\n",
    "# row-normalize (각 노드로 들어오는 부모들의 평균)\n",
    "row_sum = A_pc.sum(axis=1, keepdims=True)\n",
    "A_pc_hat = A_pc / np.maximum(row_sum, 1e-8)\n",
    "\n",
    "print(\"A_pc_hat stats:\", float(A_pc_hat.min()), float(A_pc_hat.mean()), float(A_pc_hat.max()))\n",
    "\n",
    "# propagation (매우 약하게)\n",
    "beta = 0.10   # 변화량 강도(0.05~0.15 권장)\n",
    "S0 = scores_va.T.astype(np.float32)     # (C, Nva)\n",
    "S  = S0 + beta * ((A_pc_hat @ S0) - S0) # residual 1-step\n",
    "\n",
    "scores_va_smooth2 = S.T\n",
    "\n",
    "top1_raw = np.argmax(scores_va, axis=1)\n",
    "top1_smo = np.argmax(scores_va_smooth2, axis=1)\n",
    "print(\"top1 changed ratio:\", float(np.mean(top1_raw != top1_smo)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "551df193-2aa9-4efc-8f20-089f614d50f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:26:53.086739Z",
     "iopub.status.busy": "2025-12-19T07:26:53.086422Z",
     "iopub.status.idle": "2025-12-19T07:26:53.452947Z",
     "shell.execute_reply": "2025-12-19T07:26:53.452387Z",
     "shell.execute_reply.started": "2025-12-19T07:26:53.086710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SVM raw] microF1=0.4377 sampleF1=0.4282 | TH=0.0010 MG=0.1302 | dist={2: 1100, 3: 80}\n",
      "[SVM+GNN smooth (alpha=0.2,T=1)] microF1=0.4156 sampleF1=0.4077 | TH=-0.0331 MG=0.1095 | dist={2: 1113, 3: 67}\n",
      "[SVM + Directed residual smooth (beta=0.10)] microF1=0.4442 sampleF1=0.4349 | TH=-0.0262 MG=0.1338 | dist={2: 1102, 3: 78}\n"
     ]
    }
   ],
   "source": [
    "def predict_2or3(score_mat, thresh_third_pctl=85, margin_pctl=75, margin_floor=0.05):\n",
    "    idx_sorted = np.argsort(-score_mat, axis=1)[:, :3]\n",
    "    top2 = score_mat[np.arange(score_mat.shape[0]), idx_sorted[:,1]]\n",
    "    top3 = score_mat[np.arange(score_mat.shape[0]), idx_sorted[:,2]]\n",
    "    gap23 = top2 - top3\n",
    "\n",
    "    TH = float(np.percentile(top3, thresh_third_pctl))\n",
    "    MG = float(np.percentile(gap23, margin_pctl))\n",
    "    MG = max(MG, margin_floor)\n",
    "\n",
    "    pred = []\n",
    "    for i in range(score_mat.shape[0]):\n",
    "        a,b,c = idx_sorted[i]\n",
    "        s2 = float(score_mat[i,b]); s3v = float(score_mat[i,c])\n",
    "        labs = [int(a), int(b)]\n",
    "        if (s3v >= TH) and ((s2 - s3v) <= MG):\n",
    "            labs.append(int(c))\n",
    "        pred.append(labs)\n",
    "    return pred, TH, MG\n",
    "\n",
    "def eval_scores(score_mat, name):\n",
    "    pred_labels, TH, MG = predict_2or3(score_mat, thresh_third_pctl=85, margin_pctl=75, margin_floor=0.05)\n",
    "    Ypred = np.zeros_like(Yva_bin, dtype=np.int32)\n",
    "    for i,labs in enumerate(pred_labels):\n",
    "        Ypred[i, labs] = 1\n",
    "    micro = f1_score(Yva_bin, Ypred, average=\"micro\", zero_division=0)\n",
    "    samples = f1_score(Yva_bin, Ypred, average=\"samples\", zero_division=0)\n",
    "    dist = pd.Series([len(x) for x in pred_labels]).value_counts().sort_index().to_dict()\n",
    "    print(f\"[{name}] microF1={micro:.4f} sampleF1={samples:.4f} | TH={TH:.4f} MG={MG:.4f} | dist={dist}\")\n",
    "    return {\"micro\": micro, \"samples\": samples, \"TH\": TH, \"MG\": MG, \"dist\": dist}\n",
    "\n",
    "res_raw = eval_scores(scores_va, \"SVM raw\")\n",
    "res_smo = eval_scores(scores_va_smooth, f\"SVM+GNN smooth (alpha={alpha},T={T})\")\n",
    "res_smo2 = eval_scores(scores_va_smooth2, \"SVM + Directed residual smooth (beta=0.10)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "644f6796-53e6-4c6a-99ba-91be068585a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:27:46.875000Z",
     "iopub.status.busy": "2025-12-19T07:27:46.874702Z",
     "iopub.status.idle": "2025-12-19T07:27:46.880949Z",
     "shell.execute_reply": "2025-12-19T07:27:46.880469Z",
     "shell.execute_reply.started": "2025-12-19T07:27:46.874976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/step5_gnn_directed_residual_meta.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "ART.mkdir(exist_ok=True)\n",
    "\n",
    "step5_meta = {\n",
    "    \"method\": \"GNN-based directed residual smoothing on taxonomy (post-process Step4 scores)\",\n",
    "    \"graph\": {\n",
    "        \"type\": \"parent_to_child_row_normalized_with_self_loop\",\n",
    "        \"num_nodes\": int(NUM_CLASSES),\n",
    "        \"num_edges\": int(len(edges)),\n",
    "    },\n",
    "    \"smoothing\": {\n",
    "        \"beta\": 0.10,\n",
    "        \"formula\": \"S = S0 + beta * (A_pc_hat @ S0 - S0)\",\n",
    "        \"note\": \"Directed message passing (parent->child) + residual to preserve original scores\"\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"svm_raw\": {\"micro_f1\": float(res_raw[\"micro\"]), \"sample_f1\": float(res_raw[\"samples\"]),\n",
    "                    \"TH\": float(res_raw[\"TH\"]), \"MG\": float(res_raw[\"MG\"]), \"dist\": res_raw[\"dist\"]},\n",
    "        \"smooth_bidirectional_alpha02_T1\": {\"micro_f1\": float(res_smo[\"micro\"]), \"sample_f1\": float(res_smo[\"samples\"]),\n",
    "                    \"TH\": float(res_smo[\"TH\"]), \"MG\": float(res_smo[\"MG\"]), \"dist\": res_smo[\"dist\"]},\n",
    "        \"smooth_directed_residual_beta010\": {\"micro_f1\": float(res_smo2[\"micro\"]), \"sample_f1\": float(res_smo2[\"samples\"]),\n",
    "                    \"TH\": float(res_smo2[\"TH\"]), \"MG\": float(res_smo2[\"MG\"]), \"dist\": res_smo2[\"dist\"]},\n",
    "    }\n",
    "}\n",
    "\n",
    "out = ART / \"step5_gnn_directed_residual_meta.json\"\n",
    "with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(step5_meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ecc079-13b6-4f8e-97af-883ec70aac1a",
   "metadata": {},
   "source": [
    "### Step 6: Self-trainng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cecc184e-6e85-429d-abbb-e35f31a6ffba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:29:19.526953Z",
     "iopub.status.busy": "2025-12-19T07:29:19.526688Z",
     "iopub.status.idle": "2025-12-19T07:29:20.392906Z",
     "shell.execute_reply": "2025-12-19T07:29:20.392349Z",
     "shell.execute_reply.started": "2025-12-19T07:29:19.526933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Step4 model.\n",
      "train_all: (29487, 3) test_all: (19658, 3)\n",
      "base train: (26538, 3) val_hold: (2949, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "\n",
    "# Step4 모델\n",
    "bundle = joblib.load(ART / \"step4_tfidf_linearSVC_ovr.joblib\")\n",
    "vec = bundle[\"vectorizer\"]\n",
    "clf = bundle[\"model\"]\n",
    "mlb = bundle[\"mlb\"]\n",
    "\n",
    "print(\"Loaded Step4 model.\")\n",
    "\n",
    "# 전체 train/test (Step1에서 만든 train_df/test_df 사용)\n",
    "assert \"train_df\" in globals() and \"test_df\" in globals()\n",
    "train_all = train_df.copy()\n",
    "test_all  = test_df.copy()\n",
    "train_all[\"doc_id\"] = train_all[\"doc_id\"].astype(str)\n",
    "test_all[\"doc_id\"]  = test_all[\"doc_id\"].astype(str)\n",
    "\n",
    "print(\"train_all:\", train_all.shape, \"test_all:\", test_all.shape)\n",
    "\n",
    "# Step6에서도 동일하게 val split을 유지(재현성)\n",
    "SEED = 42\n",
    "VAL_RATIO = 0.1\n",
    "idx_all = np.arange(len(train_all))\n",
    "tr_idx, va_idx = train_test_split(idx_all, test_size=VAL_RATIO, random_state=SEED, shuffle=True)\n",
    "\n",
    "train_base = train_all.iloc[tr_idx].reset_index(drop=True)\n",
    "val_hold   = train_all.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"base train:\", train_base.shape, \"val_hold:\", val_hold.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c98040e9-c7a2-4718-b5ba-44b07255df48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:35:43.192297Z",
     "iopub.status.busy": "2025-12-19T07:35:43.192019Z",
     "iopub.status.idle": "2025-12-19T07:35:43.574889Z",
     "shell.execute_reply": "2025-12-19T07:35:43.574233Z",
     "shell.execute_reply.started": "2025-12-19T07:35:43.192275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed merged: (26538, 5)\n",
      "seed label missing: 0\n",
      "seed_labeled: (26538, 5)\n",
      "CONF_THR: 0.05300162082\n",
      "seed_init: (10615, 5)\n",
      "labels per doc: {2: 5602, 3: 5013}\n",
      "pool_low (lowconf): (15923, 5)\n",
      "pool_low labels per doc: {2: 5928, 3: 9995}\n",
      "pool_low confidence describe:\n",
      "count    15923.000000\n",
      "mean         0.031113\n",
      "std          0.013627\n",
      "min          0.000000\n",
      "25%          0.022034\n",
      "50%          0.032297\n",
      "75%          0.042052\n",
      "max          0.053001\n",
      "Name: confidence, dtype: float64\n",
      "eval_hold: (1061, 5) | seed_init(after drop): (9554, 5)\n"
     ]
    }
   ],
   "source": [
    "silver_all = pd.read_json(ART / \"silver_train_tfidf_all.jsonl\", lines=True)\n",
    "silver_all[\"doc_id\"] = silver_all[\"doc_id\"].astype(str)\n",
    "\n",
    "# base train에 붙이기\n",
    "seed = train_base.merge(silver_all[[\"doc_id\",\"labels\",\"confidence\"]], on=\"doc_id\", how=\"left\")\n",
    "\n",
    "print(\"seed merged:\", seed.shape)\n",
    "print(\"seed label missing:\", int(seed[\"labels\"].isna().sum()))\n",
    "\n",
    "# seed label이 있는 것만을 초기 학습 데이터로(초기엔 highconf만 사용 권장)\n",
    "seed_labeled = seed.dropna(subset=[\"labels\"]).copy()\n",
    "seed_labeled[\"labels\"] = seed_labeled[\"labels\"].apply(lambda x: list(x) if isinstance(x, (list,tuple)) else x)\n",
    "\n",
    "print(\"seed_labeled:\", seed_labeled.shape)\n",
    "\n",
    "# 초기 학습은 highconf 기준으로 필터(노이즈 억제)\n",
    "CONF_THR = float(np.percentile(seed_labeled[\"confidence\"].values, 60))  # Step3와 동일 컨셉\n",
    "seed_init = seed_labeled[seed_labeled[\"confidence\"] >= CONF_THR].copy()\n",
    "\n",
    "print(\"CONF_THR:\", CONF_THR)\n",
    "print(\"seed_init:\", seed_init.shape)\n",
    "print(\"labels per doc:\", seed_init[\"labels\"].apply(len).value_counts().sort_index().to_dict())\n",
    "\n",
    "\n",
    "# === ADD at end of Cell 2 ===\n",
    "pool_low = seed_labeled[seed_labeled[\"confidence\"] < CONF_THR].copy()\n",
    "print(\"pool_low (lowconf):\", pool_low.shape)\n",
    "print(\"pool_low labels per doc:\", pool_low[\"labels\"].apply(len).value_counts().sort_index().to_dict())\n",
    "print(\"pool_low confidence describe:\")\n",
    "print(pool_low[\"confidence\"].describe())\n",
    "\n",
    "# ===== Eval holdout (pseudo-GT) split to avoid leakage =====\n",
    "EVAL_PCT = 10  # seed_init의 10%를 평가 전용으로 분리\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "eval_idx = rng.choice(seed_init.index.values, size=int(len(seed_init) * EVAL_PCT/100), replace=False)\n",
    "eval_hold = seed_init.loc[eval_idx].copy()\n",
    "seed_init = seed_init.drop(eval_idx).copy()\n",
    "\n",
    "print(\"eval_hold:\", eval_hold.shape, \"| seed_init(after drop):\", seed_init.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29a8b305-0168-4618-8cad-24639d4a927a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:29:33.275753Z",
     "iopub.status.busy": "2025-12-19T07:29:33.275477Z",
     "iopub.status.idle": "2025-12-19T07:29:33.283291Z",
     "shell.execute_reply": "2025-12-19T07:29:33.282791Z",
     "shell.execute_reply.started": "2025-12-19T07:29:33.275732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step5에서 확정한 directed residual smoothing(beta=0.10)\n",
    "beta = 0.10\n",
    "\n",
    "# taxonomy edges -> A_pc_hat 구성 (parent->child)\n",
    "N = len(classes_df)\n",
    "A_pc = np.zeros((N, N), dtype=np.float32)\n",
    "for p, c in edges:\n",
    "    A_pc[c, p] = 1.0\n",
    "\n",
    "A_pc[np.arange(N), np.arange(N)] = 1.0\n",
    "row_sum = A_pc.sum(axis=1, keepdims=True)\n",
    "A_pc_hat = A_pc / np.maximum(row_sum, 1e-8)\n",
    "\n",
    "def smooth_scores_parent_to_child(scores, beta=0.10):\n",
    "    # scores: (num_docs, num_classes)\n",
    "    S0 = scores.T.astype(np.float32)             # (C,N)\n",
    "    S  = S0 + beta * ((A_pc_hat @ S0) - S0)      # residual 1-step\n",
    "    return S.T\n",
    "\n",
    "def pick_2or3_from_scores(score_mat, thresh_third_pctl=85, margin_pctl=75, margin_floor=0.05):\n",
    "    idx_sorted = np.argsort(-score_mat, axis=1)[:, :3]\n",
    "    top2 = score_mat[np.arange(score_mat.shape[0]), idx_sorted[:,1]]\n",
    "    top3 = score_mat[np.arange(score_mat.shape[0]), idx_sorted[:,2]]\n",
    "    gap23 = top2 - top3\n",
    "\n",
    "    TH = float(np.percentile(top3, thresh_third_pctl))\n",
    "    MG = float(np.percentile(gap23, margin_pctl))\n",
    "    MG = max(MG, margin_floor)\n",
    "\n",
    "    pred = []\n",
    "    conf = []\n",
    "    for i in range(score_mat.shape[0]):\n",
    "        a,b,c = idx_sorted[i]\n",
    "        s1 = float(score_mat[i,a]); s2 = float(score_mat[i,b]); s3 = float(score_mat[i,c])\n",
    "        labs = [int(a), int(b)]\n",
    "        if (s3 >= TH) and ((s2 - s3) <= MG):\n",
    "            labs.append(int(c))\n",
    "        pred.append(labs)\n",
    "\n",
    "        # confidence: top2 평균 + (top2-top4 유사 대용으로 top2-top3) 양수 가중\n",
    "        conf_i = (s1 + s2)/2.0 + 0.25 * max(0.0, (s2 - s3))\n",
    "        conf.append(conf_i)\n",
    "\n",
    "    return pred, np.array(conf, dtype=np.float32), TH, MG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "708208f5-b134-4794-8826-a97d0468025a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:36:03.377258Z",
     "iopub.status.busy": "2025-12-19T07:36:03.376955Z",
     "iopub.status.idle": "2025-12-19T07:36:06.566698Z",
     "shell.execute_reply": "2025-12-19T07:36:06.565926Z",
     "shell.execute_reply.started": "2025-12-19T07:36:03.377236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool_low: (15923, 5)\n",
      "TH/MG: -0.05106486417353153 0.05\n",
      "pseudo_conf2 describe:\n",
      "count    15923.000000\n",
      "mean         0.169723\n",
      "std          0.150168\n",
      "min         -0.086586\n",
      "25%          0.073364\n",
      "50%          0.173585\n",
      "75%          0.251312\n",
      "max          2.031942\n",
      "Name: pseudo_conf2, dtype: float64\n",
      "overlap distribution: {0: 14073, 1: 1284, 2: 515, 3: 51}\n",
      "ADD_TOP_PCT: 5 | thr_conf: 0.32395256757736207\n",
      "to_add: (544, 8)\n",
      "labels per doc (to_add): {2: 484, 3: 60}\n",
      "\n",
      "[Sanity check: original vs pseudo]\n",
      "doc_id: 23457 orig: [220, 129] pseudo: [220, 439] overlap: 1 top1: 0.6037007570266724 conf2: 0.5777121186256409\n",
      "doc_id: 14658 orig: [220, 221] pseudo: [220, 221] overlap: 2 top1: 1.0881788730621338 conf2: 1.7485592365264893\n",
      "doc_id: 14614 orig: [294, 433, 435] pseudo: [294, 433] overlap: 2 top1: 0.8134709000587463 conf2: 1.3495101928710938\n",
      "doc_id: 12290 orig: [220, 221] pseudo: [220, 221] overlap: 2 top1: 0.6159065961837769 conf2: 0.6466577053070068\n",
      "doc_id: 7136 orig: [15, 166] pseudo: [166, 15] overlap: 2 top1: 0.4338575601577759 conf2: 0.7830818891525269\n",
      "doc_id: 28006 orig: [171, 258, 96] pseudo: [171, 383] overlap: 1 top1: 0.4046309292316437 conf2: 0.37846633791923523\n",
      "doc_id: 7249 orig: [90, 461, 473] pseudo: [473, 90] overlap: 2 top1: 0.23193615674972534 conf2: 0.3688866198062897\n",
      "doc_id: 24823 orig: [61, 30, 104] pseudo: [61, 383] overlap: 1 top1: 0.9041465520858765 conf2: 0.8778374791145325\n"
     ]
    }
   ],
   "source": [
    "# pool = low-confidence portion\n",
    "pool = pool_low.copy()\n",
    "print(\"pool_low:\", pool.shape)\n",
    "\n",
    "# teacher scores\n",
    "X_pool_vec = vec.transform(pool[\"text\"].astype(str).tolist())\n",
    "scores_pool = np.asarray(clf.decision_function(X_pool_vec))\n",
    "\n",
    "# Step5 directed residual smoothing\n",
    "scores_pool_s = smooth_scores_parent_to_child(scores_pool, beta=beta)\n",
    "\n",
    "# top-k 정보로 더 안정적인 confidence 구성\n",
    "idx_sorted = np.argsort(-scores_pool_s, axis=1)[:, :4]  # top4까지\n",
    "top1 = scores_pool_s[np.arange(scores_pool_s.shape[0]), idx_sorted[:,0]]\n",
    "top2 = scores_pool_s[np.arange(scores_pool_s.shape[0]), idx_sorted[:,1]]\n",
    "top3 = scores_pool_s[np.arange(scores_pool_s.shape[0]), idx_sorted[:,2]]\n",
    "top4 = scores_pool_s[np.arange(scores_pool_s.shape[0]), idx_sorted[:,3]]\n",
    "\n",
    "# 예측 라벨(2~3개)\n",
    "pred_labels, _, TH, MG = pick_2or3_from_scores(scores_pool_s, thresh_third_pctl=85, margin_pctl=75)\n",
    "\n",
    "# 새로운 confidence: (절대값 + 마진) 기반 (LinearSVC score 스케일에 덜 민감)\n",
    "pseudo_conf2 = top1 + 0.5*(top2) + 0.5*(top2 - top4)  # top1 크기 + 상위 분리도\n",
    "\n",
    "pool[\"pseudo_labels\"] = pred_labels\n",
    "pool[\"pseudo_conf2\"] = pseudo_conf2\n",
    "\n",
    "print(\"TH/MG:\", TH, MG)\n",
    "print(\"pseudo_conf2 describe:\")\n",
    "print(pd.Series(pool[\"pseudo_conf2\"]).describe())\n",
    "\n",
    "# ===== Consistency filter: 기존 labels와 pseudo_labels가 최소 1개 이상 겹치기 =====\n",
    "def overlap_count(a, b):\n",
    "    return len(set(a).intersection(set(b)))\n",
    "\n",
    "pool[\"overlap\"] = [overlap_count(o, p) for o, p in zip(pool[\"labels\"], pool[\"pseudo_labels\"])]\n",
    "\n",
    "print(\"overlap distribution:\", pool[\"overlap\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "# ===== 매우 엄격하게 추가: (1) top1 > 0 조건 + (2) pseudo_conf2 상위 5% + (3) overlap>=1 =====\n",
    "ADD_TOP_PCT = 5\n",
    "thr_conf = float(np.percentile(pool[\"pseudo_conf2\"].values, 100-ADD_TOP_PCT))\n",
    "\n",
    "to_add = pool[\n",
    "    (top1 > 0) &\n",
    "    (pool[\"pseudo_conf2\"] >= thr_conf) &\n",
    "    (pool[\"overlap\"] >= 1)\n",
    "].copy()\n",
    "\n",
    "print(\"ADD_TOP_PCT:\", ADD_TOP_PCT, \"| thr_conf:\", thr_conf)\n",
    "print(\"to_add:\", to_add.shape)\n",
    "print(\"labels per doc (to_add):\", to_add[\"pseudo_labels\"].apply(len).value_counts().sort_index().to_dict())\n",
    "\n",
    "print(\"\\n[Sanity check: original vs pseudo]\")\n",
    "for i in range(min(8, len(to_add))):\n",
    "    r = to_add.iloc[i]\n",
    "    print(\"doc_id:\", r[\"doc_id\"], \"orig:\", r[\"labels\"], \"pseudo:\", r[\"pseudo_labels\"],\n",
    "          \"overlap:\", int(r[\"overlap\"]), \"top1:\", float(top1[pool.index.get_loc(r.name)]), \"conf2:\", float(r[\"pseudo_conf2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8ebdd7d5-b172-40eb-b380-4d901c65ab95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:37:00.468956Z",
     "iopub.status.busy": "2025-12-19T07:37:00.468674Z",
     "iopub.status.idle": "2025-12-19T07:37:04.941029Z",
     "shell.execute_reply": "2025-12-19T07:37:04.940339Z",
     "shell.execute_reply.started": "2025-12-19T07:37:00.468933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mix: (10098, 2) | pseudo added: 544\n",
      "trained self-training model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "train_mix = pd.concat([\n",
    "    seed_init[[\"text\",\"labels\"]].rename(columns={\"labels\":\"y\"}),\n",
    "    to_add[[\"text\",\"pseudo_labels\"]].rename(columns={\"pseudo_labels\":\"y\"})\n",
    "], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(\"train_mix:\", train_mix.shape, \"| pseudo added:\", len(to_add))\n",
    "\n",
    "X_mix = vec.transform(train_mix[\"text\"].astype(str).tolist())\n",
    "Y_mix = mlb.transform(train_mix[\"y\"].tolist())\n",
    "\n",
    "clf_st = OneVsRestClassifier(LinearSVC(C=1.0), n_jobs=-1)\n",
    "clf_st.fit(X_mix, Y_mix)\n",
    "\n",
    "print(\"trained self-training model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2699f96a-7e09-4f89-8046-c0eaca7fd4bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:39:26.455244Z",
     "iopub.status.busy": "2025-12-19T07:39:26.454929Z",
     "iopub.status.idle": "2025-12-19T07:39:29.756473Z",
     "shell.execute_reply": "2025-12-19T07:39:29.755899Z",
     "shell.execute_reply.started": "2025-12-19T07:39:26.455216Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/esci/lib/python3.9/site-packages/sklearn/multiclass.py:77: UserWarning: Label not 383 is present in all training examples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/esci/lib/python3.9/site-packages/sklearn/multiclass.py:77: UserWarning: Label not 439 is present in all training examples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/esci/lib/python3.9/site-packages/sklearn/multiclass.py:77: UserWarning: Label not 505 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained fair baseline on seed_init (eval_hold excluded): (9554, 55968) (9554, 531)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "\n",
    "# seed_init은 이미 eval_hold 제거된 상태: 9554\n",
    "X_base = vec.transform(seed_init[\"text\"].astype(str).tolist())\n",
    "Y_base = mlb.transform(seed_init[\"labels\"].tolist())\n",
    "\n",
    "clf_base_fair = OneVsRestClassifier(LinearSVC(C=1.0), n_jobs=-1)\n",
    "clf_base_fair.fit(X_base, Y_base)\n",
    "\n",
    "print(\"trained fair baseline on seed_init (eval_hold excluded):\", X_base.shape, Y_base.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d35a603c-f2a2-453b-9df9-c5e4a0e986d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:39:41.614641Z",
     "iopub.status.busy": "2025-12-19T07:39:41.614341Z",
     "iopub.status.idle": "2025-12-19T07:39:44.956807Z",
     "shell.execute_reply": "2025-12-19T07:39:44.956100Z",
     "shell.execute_reply.started": "2025-12-19T07:39:41.614616Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/esci/lib/python3.9/site-packages/sklearn/multiclass.py:77: UserWarning: Label not 383 is present in all training examples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/esci/lib/python3.9/site-packages/sklearn/multiclass.py:77: UserWarning: Label not 439 is present in all training examples.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/esci/lib/python3.9/site-packages/sklearn/multiclass.py:77: UserWarning: Label not 505 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained fair baseline on seed_init (eval_hold excluded): (9554, 55968) (9554, 531)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "\n",
    "# seed_init은 이미 eval_hold 제거된 상태: 9554\n",
    "X_base = vec.transform(seed_init[\"text\"].astype(str).tolist())\n",
    "Y_base = mlb.transform(seed_init[\"labels\"].tolist())\n",
    "\n",
    "clf_base_fair = OneVsRestClassifier(LinearSVC(C=1.0), n_jobs=-1)\n",
    "clf_base_fair.fit(X_base, Y_base)\n",
    "\n",
    "print(\"trained fair baseline on seed_init (eval_hold excluded):\", X_base.shape, Y_base.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f27ec53-a052-471a-9bd5-fa8afdb127d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:40:25.686684Z",
     "iopub.status.busy": "2025-12-19T07:40:25.686260Z",
     "iopub.status.idle": "2025-12-19T07:40:26.362773Z",
     "shell.execute_reply": "2025-12-19T07:40:26.361755Z",
     "shell.execute_reply.started": "2025-12-19T07:40:25.686647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval on eval_hold (no-leak fair comparison)\n",
      " fair baseline(seed_init only) micro=0.4484 sample=0.4382\n",
      " self-train(+to_add)        micro=0.7209 sample=0.7183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "Xv = vec.transform(eval_hold[\"text\"].astype(str).tolist())\n",
    "Yv = mlb.transform(eval_hold[\"labels\"].tolist())\n",
    "\n",
    "# fair baseline\n",
    "sc_base = np.asarray(clf_base_fair.decision_function(Xv))\n",
    "sc_base = smooth_scores_parent_to_child(sc_base, beta=beta)\n",
    "pred_b, _, _, _ = pick_2or3_from_scores(sc_base)\n",
    "Ypb = np.zeros_like(Yv, dtype=np.int32)\n",
    "for i,labs in enumerate(pred_b):\n",
    "    Ypb[i, labs] = 1\n",
    "\n",
    "# self-trained\n",
    "sc_st = np.asarray(clf_st.decision_function(Xv))\n",
    "sc_st = smooth_scores_parent_to_child(sc_st, beta=beta)\n",
    "pred_s, _, _, _ = pick_2or3_from_scores(sc_st)\n",
    "Yps = np.zeros_like(Yv, dtype=np.int32)\n",
    "for i,labs in enumerate(pred_s):\n",
    "    Yps[i, labs] = 1\n",
    "\n",
    "micro_base = f1_score(Yv, Ypb, average=\"micro\", zero_division=0)\n",
    "micro_st   = f1_score(Yv, Yps, average=\"micro\", zero_division=0)\n",
    "samp_base  = f1_score(Yv, Ypb, average=\"samples\", zero_division=0)\n",
    "samp_st    = f1_score(Yv, Yps, average=\"samples\", zero_division=0)\n",
    "\n",
    "print(\"Eval on eval_hold (no-leak fair comparison)\")\n",
    "print(f\" fair baseline(seed_init only) micro={micro_base:.4f} sample={samp_base:.4f}\")\n",
    "print(f\" self-train(+to_add)        micro={micro_st:.4f} sample={samp_st:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7c3bd-d164-4f81-9357-6124291aaccf",
   "metadata": {},
   "source": [
    "### Step7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5bd636b-d8e4-482f-b4ce-ac79e67e81e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:41:17.756898Z",
     "iopub.status.busy": "2025-12-19T07:41:17.756473Z",
     "iopub.status.idle": "2025-12-19T07:41:20.637803Z",
     "shell.execute_reply": "2025-12-19T07:41:20.637161Z",
     "shell.execute_reply.started": "2025-12-19T07:41:17.756876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_test: (19658, 531) scores_test_s: (19658, 531)\n",
      "scores_test_s min/mean/max: -2.487013101577759 -1.0674573183059692 2.880851984024048\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# test_all: (19658, 3) already loaded in Step6 Cell1\n",
    "X_test = vec.transform(test_all[\"text\"].astype(str).tolist())\n",
    "\n",
    "scores_test = np.asarray(clf_st.decision_function(X_test))   # (Ntest, 531)\n",
    "scores_test_s = smooth_scores_parent_to_child(scores_test, beta=beta)\n",
    "\n",
    "print(\"scores_test:\", scores_test.shape, \"scores_test_s:\", scores_test_s.shape)\n",
    "print(\"scores_test_s min/mean/max:\", float(scores_test_s.min()), float(scores_test_s.mean()), float(scores_test_s.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b70c47f6-a670-4e0f-9178-6c93015fe0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:41:25.675469Z",
     "iopub.status.busy": "2025-12-19T07:41:25.675054Z",
     "iopub.status.idle": "2025-12-19T07:41:26.227233Z",
     "shell.execute_reply": "2025-12-19T07:41:26.226741Z",
     "shell.execute_reply.started": "2025-12-19T07:41:25.675434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TH/MG (test): -0.34720872044563283 0.24121679738163948\n",
      "pred labels per doc distribution: {2: 17842, 3: 1816}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[90, 473]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[168, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[153, 140]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[314, 373]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[67, 510]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[206, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[397, 313]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[161, 115]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[49, 264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[81, 48]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id      labels\n",
       "0       0   [90, 473]\n",
       "1       1   [168, 18]\n",
       "2       2  [153, 140]\n",
       "3       3  [314, 373]\n",
       "4       4   [67, 510]\n",
       "5       5    [206, 8]\n",
       "6       6  [397, 313]\n",
       "7       7  [161, 115]\n",
       "8       8   [49, 264]\n",
       "9       9    [81, 48]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test, _, THt, MGt = pick_2or3_from_scores(scores_test_s, thresh_third_pctl=85, margin_pctl=75)\n",
    "\n",
    "lens = pd.Series([len(x) for x in pred_test]).value_counts().sort_index()\n",
    "print(\"TH/MG (test):\", THt, MGt)\n",
    "print(\"pred labels per doc distribution:\", lens.to_dict())\n",
    "\n",
    "# 미리보기\n",
    "preview = pd.DataFrame({\n",
    "    \"doc_id\": test_all[\"doc_id\"].astype(int),\n",
    "    \"labels\": pred_test\n",
    "}).head(10)\n",
    "preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5530914f-5eec-43e8-baff-3f9838024f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:45:14.704465Z",
     "iopub.status.busy": "2025-12-19T07:45:14.704208Z",
     "iopub.status.idle": "2025-12-19T07:45:14.740666Z",
     "shell.execute_reply": "2025-12-19T07:45:14.740180Z",
     "shell.execute_reply.started": "2025-12-19T07:45:14.704445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/2021350218_final.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>90, 473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>168, 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>153, 140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>314, 373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>67, 510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    labels\n",
       "0   0   90, 473\n",
       "1   1   168, 18\n",
       "2   2  153, 140\n",
       "3   3  314, 373\n",
       "4   4   67, 510"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT = Path(\"artifacts\") / \"2021350218_final.csv\"\n",
    "OUT.parent.mkdir(exist_ok=True)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": test_all[\"doc_id\"].astype(int),\n",
    "    \"labels\": [\", \".join(map(str, labs)) for labs in pred_test]\n",
    "})\n",
    "\n",
    "sub.to_csv(OUT, index=False)\n",
    "print(\"Saved:\", OUT)\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e87bf953-284b-4db9-922d-a35ebe6ee784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T07:42:57.887952Z",
     "iopub.status.busy": "2025-12-19T07:42:57.887567Z",
     "iopub.status.idle": "2025-12-19T07:42:57.899856Z",
     "shell.execute_reply": "2025-12-19T07:42:57.899342Z",
     "shell.execute_reply.started": "2025-12-19T07:42:57.887932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts/final_pipeline_meta.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "ART.mkdir(exist_ok=True)\n",
    "\n",
    "final_meta = {\n",
    "    \"data\": {\n",
    "        \"num_classes\": int(NUM_CLASSES),\n",
    "        \"num_edges\": int(len(edges)),\n",
    "        \"num_train\": int(len(train_all)),\n",
    "        \"num_test\": int(len(test_all)),\n",
    "        \"seed_init_size\": int(len(seed_init)),\n",
    "        \"eval_hold_size\": int(len(eval_hold)),\n",
    "        \"pool_low_size\": int(len(pool_low)),\n",
    "        \"to_add_size\": int(len(to_add)),\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"baseline_step4\": \"LinearSVC OneVsRest + TFIDF\",\n",
    "        \"taxonomy_step5\": \"Directed residual smoothing (parent->child) beta=0.10\",\n",
    "        \"self_training_step6\": \"Re-train on seed_init + filtered pseudo from lowconf pool\",\n",
    "    },\n",
    "    \"step5\": {\n",
    "        \"beta\": float(beta),\n",
    "        \"formula\": \"S = S0 + beta * (A_pc_hat @ S0 - S0)\",\n",
    "    },\n",
    "    \"step6_selection\": {\n",
    "        \"CONF_THR_seed\": float(CONF_THR),\n",
    "        \"ADD_TOP_PCT\": int(ADD_TOP_PCT),\n",
    "        \"filters\": [\"top1>0\", \"pseudo_conf2>=thr_conf(top5%)\", \"overlap>=1\"],\n",
    "    },\n",
    "    \"eval_no_leak\": {\n",
    "        \"fair_baseline_micro\": float(micro_base),\n",
    "        \"fair_baseline_sample\": float(samp_base),\n",
    "        \"self_train_micro\": float(micro_st),\n",
    "        \"self_train_sample\": float(samp_st),\n",
    "    },\n",
    "    \"test_prediction\": {\n",
    "        \"TH_test\": float(THt),\n",
    "        \"MG_test\": float(MGt),\n",
    "        \"pred_dist\": {int(k): int(v) for k,v in pd.Series([len(x) for x in pred_test]).value_counts().sort_index().items()},\n",
    "    },\n",
    "    \"artifacts\": {\n",
    "        \"step4_model\": \"artifacts/step4_tfidf_linearSVC_ovr.joblib\",\n",
    "        \"step5_meta\": \"artifacts/step5_gnn_directed_residual_meta.json\",\n",
    "        \"submission\": \"artifacts/submission.csv\",\n",
    "    }\n",
    "}\n",
    "\n",
    "OUT_META = ART / \"final_pipeline_meta.json\"\n",
    "with open(OUT_META, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", OUT_META)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea507495-4bb4-4336-8d3e-e90d63546991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESCI Environment (Python 3.9)",
   "language": "python",
   "name": "esci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
